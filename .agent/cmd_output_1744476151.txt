Stdout/Stderr: import click
import json
import llm
import uuid
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timezone
import logging
import sys # Needed for read_stdin_if_not_tty
import re
import os
import pathlib
import sqlite_utils
from pydantic import BaseModel
import time
import concurrent.futures
import threading

# --- Database Setup ---

def user_dir() -> pathlib.Path:
    path = pathlib.Path(click.get_app_dir("io.datasette.llm"))
    path.mkdir(parents=True, exist_ok=True)
    return path

def logs_db_path() -> pathlib.Path:
    return user_dir() / "logs.db"

# Flag to track if tables have been ensured
_tables_ensured = False

def ensure_consortium_tables(db: sqlite_utils.Database):
    """Create consortium-specific tables if they don't exist."""
    global _tables_ensured
    if _tables_ensured:
        return

    logger.debug("Ensuring consortium database tables exist.")
    db["consortium_runs"].create({
        "run_id": str, # TEXT UUID
        "start_timestamp": str, # ISO 8601 TEXT
        "end_timestamp": str,
        "status": str, # 'running', 'completed', 'failed'
        "original_prompt": str,
        "consortium_config": str, # JSON TEXT
        "final_synthesis": str,
        "iterations_performed": int,
        "final_confidence": float,
        "error_message": str,
        # Future: evaluation_metrics TEXT
    }, pk="run_id", if_not_exists=True)

    db["consortium_iterations"].create({
        "iteration_id": int, # INTEGER AUTOINCREMENT
        "run_id": str, # TEXT UUID
        "iteration_number": int,
        "arbiter_response_id": str, # TEXT UUID from responses table
        "arbiter_parsed_confidence": float,
        "arbiter_parsed_needs_iteration": int, # Boolean (0/1)
    }, pk="iteration_id", foreign_keys=[
        ("run_id", "consortium_runs", "run_id"),
        # Making arbiter_response_id nullable as it might fail
        ("arbiter_response_id", "responses", "id") 
    ], if_not_exists=True)
    db["consortium_iterations"].create_index(["run_id", "iteration_number"], unique=True, if_not_exists=True)

    db["consortium_iteration_responses"].create({
        "iteration_id": int,
        "response_id": str, # TEXT UUID from responses table
    }, pk=("iteration_id", "response_id"), foreign_keys=[
        ("iteration_id", "consortium_iterations", "iteration_id"),
        ("response_id", "responses", "id") # Link to core responses table
    ], if_not_exists=True)
    
    # Also ensure consortium_configs table exists (moved from later)
    db["consortium_configs"].create({
        "name": str,
        "config": str,
        "created_at": str
    }, pk="name", defaults={"created_at": "CURRENT_TIMESTAMP"}, if_not_exists=True)

    _tables_ensured = True
    logger.debug("Consortium database tables ensured.")


class DatabaseConnection:
    _thread_local = threading.local()

    @classmethod
    def get_connection(cls) -> sqlite_utils.Database:
        if not hasattr(cls._thread_local, 'db'):
            db_path = logs_db_path()
            try:
                cls._thread_local.db = sqlite_utils.Database(db_path)
                # Ensure tables exist on first connection per thread
                ensure_consortium_tables(cls._thread_local.db)
            except Exception as e:
                logger.error(f"Failed to connect to or setup database at {db_path}: {e}")
                raise
        return cls._thread_local.db

# --- Logging Setup ---

def setup_logging() -> None:
    log_path = user_dir() / "consortium.log"
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    console_handler = logging.StreamHandler(sys.stderr)
    console_handler.setLevel(logging.ERROR)
    console_handler.setFormatter(formatter)

    try:
        file_handler = logging.FileHandler(str(log_path), mode='w')
        file_handler.setLevel(logging.DEBUG)
        file_handler.setFormatter(formatter)
    except Exception as e:
        print(f"Error setting up file logging: {e}", file=sys.stderr)
        file_handler = None

    root_logger = logging.getLogger()
    if root_logger.hasHandlers():
        root_logger.handlers.clear()
    root_logger.setLevel(logging.DEBUG)
    root_logger.addHandler(console_handler)
    if file_handler:
        root_logger.addHandler(file_handler)
    logging.getLogger("llm_consortium").setLevel(logging.DEBUG)

setup_logging()
logger = logging.getLogger(__name__)
logger.debug("llm_consortium module loaded, logging configured.")

# --- Utility Functions ---

def _read_prompt_file(filename: str) -> str:
    """Helper to read prompt files."""
    try:
        file_path = pathlib.Path(__file__).parent / filename
        if not file_path.is_file():
            logger.error(f"Prompt file not found: {file_path}")
            return ""
        with open(file_path, "r") as f:
            return f.read().strip()
    except Exception as e:
        logger.error(f"Error reading prompt file '{filename}': {e}")
        return ""

# Added back the missing function
def read_stdin_if_not_tty() -> Optional[str]:
    """Read from stdin if it's not connected to an interactive terminal."""
    if not sys.stdin.isatty():
        content = sys.stdin.read()
        logger.debug(f"Read {len(content)} bytes from stdin.")
        return content.strip()
    logger.debug("Stdin is a TTY, not reading.")
    return None


DEFAULT_SYSTEM_PROMPT = _read_prompt_file("system_prompt.txt")
ARBITER_PROMPT_TEMPLATE = _read_prompt_file("arbiter_prompt.xml")
ITERATION_PROMPT_TEMPLATE = _read_prompt_file("iteration_prompt.txt")

def _get_finish_reason(response_json: Dict[str, Any]) -> Optional[str]:
    # Simplified version
    if not isinstance(response_json, dict): return None
    for key in ['finish_reason', 'finishReason', 'stop_reason']:
        # Use .get() for safer access
        value = response_json.get(key)
        if value: return str(value).lower()
    return None

def log_response(response, model_name_for_log: str) -> Optional[str]:
    """Log model response to database and return the response ID."""
    # Reverted to simpler version - no consortium_id handling needed here.
    # It relies on the caller capturing response.id after this call.
    response_id = None # Initialize response_id
    try:
        db = DatabaseConnection.get_connection()
        response.log_to_db(db) # This populates response.id
        response_id = getattr(response, 'id', None) # Get ID after logging
        if response_id:
            logger.debug(f"Logged response for '{model_name_for_log}'. Response ID: {response_id}")
        else:
            logger.warning(f"Failed to get response ID after logging for '{model_name_for_log}'.")


        # Check for truncation
        if response.response_json:
            finish_reason = _get_finish_reason(response.response_json)
            # More robust check for truncation reasons
            if finish_reason and any(reason in finish_reason for reason in ['length', 'max_tokens']):
                logger.warning(f"Response from {model_name_for_log} (ID: {response_id}) may be truncated. Reason: {finish_reason}")
        
        return response_id # Return the ID generated by log_to_db (or None)
    except Exception as e:
        logger.exception(f"Error logging response for {model_name_for_log}: {e}")
        return None # Return None on error

# --- Core Logic Classes ---

class IterationResult(BaseModel):
    """Holds results from a single iteration's model calls."""
    model: str
    instance: int
    response_text: Optional[str] = None
    confidence: Optional[float] = None
    conversation_id: Optional[str] = None
    response_id: Optional[str] = None # The ID from the 'responses' table
    error: Optional[str] = None

class ArbiterSynthesisResult(BaseModel):
    """Holds the parsed result from the arbiter."""
    synthesis: str = "No synthesis found."
    confidence: float = 0.0
    analysis: str = "No analysis found."
    dissent: str = ""
    needs_iteration: bool = True
    refinement_areas: List[str] = []
    response_id: Optional[str] = None # The ID from the 'responses' table
    error: Optional[str] = None # Add error field

class ConsortiumConfig(BaseModel):
    models: Dict[str, int] # name -> count
    system_prompt: Optional[str] = None
    confidence_threshold: float = 0.8
    max_iterations: int = 3
    minimum_iterations: int = 1
    arbiter: Optional[str] = None

    def to_json(self) -> str:
        # Use model_dump_json for Pydantic v2
        return self.model_dump_json()

    @classmethod
    def from_json(cls, json_str: str):
        return cls.model_validate_json(json_str)


class ConsortiumOrchestrator:
    def __init__(self, config: ConsortiumConfig):
        self.config = config
        self.arbiter_model_name = config.arbiter or "claude-3.5-sonnet" # Sensible default
        # Track conversation IDs per model instance for continuity
        self.conversation_ids: Dict[str, Optional[str]] = {}
        self.db = DatabaseConnection.get_connection() # Get DB connection

    def orchestrate(self, prompt: str) -> Dict[str, Any]:
        run_id = str(uuid.uuid4())
        start_time_iso = datetime.now(timezone.utc).isoformat()
        start_time_perf = time.perf_counter()
        logger.info(f"Starting consortium run ID: {run_id}")

        # Create initial run record
        self.db["consortium_runs"].insert({
            "run_id": run_id,
            "start_timestamp": start_time_iso,
            "status": "running",
            "original_prompt": prompt,
            "consortium_config": self.config.to_json(),
        }, pk="run_id")

        iteration_count = 0
        final_synthesis_result = None
        all_iteration_contexts = [] # Store full context for final output

        current_prompt = prompt # Start with the original prompt for the first iteration models

        # Apply system prompt only for the first iteration models
        if self.config.system_prompt:
             current_prompt_for_models_iter1 = f"[SYSTEM INSTRUCTIONS]
{self.config.system_prompt}
[/SYSTEM INSTRUCTIONS]

{current_prompt}"
        else:
             current_prompt_for_models_iter1 = current_prompt

        try:
            while True: # Loop controlled by break conditions
                iteration_count += 1
                iter_start_time_iso = datetime.now(timezone.utc).isoformat()
                logger.debug(f"Starting Iteration {iteration_count} for Run ID: {run_id}")
                
                # Determine prompt for models in this iteration
                prompt_for_current_iter_models = current_prompt_for_models_iter1 if iteration_count == 1 else current_prompt

                # 1. Get Model Responses
                model_responses: List[IterationResult] = self._get_model_responses(prompt_for_current_iter_models, run_id, iteration_count)

                # 2. Synthesize with Arbiter
                # Arbiter always gets the original prompt plus history
                synthesis_result: ArbiterSynthesisResult = self._synthesize_responses(prompt, model_responses, all_iteration_contexts, run_id, iteration_count)

                # 3. Store Iteration Data
                iteration_db_id = self._store_iteration_data(run_id, iteration_count, model_responses, synthesis_result)
                
                # Add context for history formatting in next arbiter call & final output
                all_iteration_contexts.append({
                    "iteration_number": iteration_count,
                    "model_responses": [r.model_dump(exclude_none=True) for r in model_responses], # Store serializable data
                    "synthesis": synthesis_result.model_dump(exclude_none=True) # Store serializable data
                })

                # 4. Check Completion Conditions
                # Use >= min_iterations check
                if iteration_count >= self.config.minimum_iterations:
                     # Check confidence threshold
                     if synthesis_result.confidence >= self.config.confidence_threshold:
                          # Check if arbiter wants to stop
                          if not synthesis_result.needs_iteration:
                               logger.info(f"Run {run_id} completed: Met confidence threshold ({synthesis_result.confidence}) and arbiter needs_iteration=False at iteration {iteration_count}.")
                               final_synthesis_result = synthesis_result
                               break
                          else:
                               logger.debug(f"Run {run_id} Iter {iteration_count}: Met confidence, but arbiter needs_iteration=True.")
                     else:
                          logger.debug(f"Run {run_id} Iter {iteration_count}: Confidence ({synthesis_result.confidence}) below threshold ({self.config.confidence_threshold}).")
                else:
                     logger.debug(f"Run {run_id} Iter {iteration_count}: Below minimum iterations ({self.config.minimum_iterations}).")


                # Check max iterations
                if iteration_count >= self.config.max_iterations:
                    logger.info(f"Run {run_id} completed: Reached max iterations ({self.config.max_iterations}).")
                    final_synthesis_result = synthesis_result # Use the last synthesis
                    break

                # 5. Prepare for Next Iteration (only if loop continues)
                logger.debug(f"Run {run_id} preparing for iteration {iteration_count + 1}.")
                # Models in next iteration get prompt based on arbiter feedback
                current_prompt = self._construct_iteration_prompt(prompt, synthesis_result)

        except Exception as e:
            logger.exception(f"Error during orchestration for run {run_id}")
            # Update run status to failed
            self.db["consortium_runs"].update(run_id, {
                "status": "failed",
                "error_message": str(e),
                "end_timestamp": datetime.now(timezone.utc).isoformat(),
            }, alter=True) # Use alter=True to add column if missing (though ensure_tables should handle)
            # Re-raise or return error structure
            raise llm.ModelError(f"Consortium run {run_id} failed: {e}") from e


        # Finalize run record
        end_time_iso = datetime.now(timezone.utc).isoformat()
        total_duration_ms = int((time.perf_counter() - start_time_perf) * 1000)

        # Ensure final_synthesis_result is not None before accessing attributes
        final_synth_text = "N/A"
        final_conf = None
        if final_synthesis_result:
            final_synth_text = final_synthesis_result.synthesis
            final_conf = final_synthesis_result.confidence

        self.db["consortium_runs"].update(run_id, {
            "status": "completed",
            "end_timestamp": end_time_iso,
            "iterations_performed": iteration_count,
            "final_synthesis": final_synth_text,
            "final_confidence": final_conf,
        }, alter=True)
        
        logger.info(f"Consortium run {run_id} finished successfully in {total_duration_ms}ms.")

        # Construct final output payload (similar to before but using stored history)
        return {
            "original_prompt": prompt,
            "synthesis": final_synthesis_result.model_dump(exclude_none=True) if final_synthesis_result else {},
            "iteration_history": all_iteration_contexts,
            "metadata": {
                "run_id": run_id,
                "models_used": self.config.models,
                "arbiter": self.arbiter_model_name,
                "start_timestamp": start_time_iso,
                "end_timestamp": end_time_iso,
                "total_time_ms": total_duration_ms,
                "iteration_count": iteration_count,
                "confidence_threshold": self.config.confidence_threshold,
                "max_iterations": self.config.max_iterations,
                "min_iterations": self.config.minimum_iterations
            }
        }

    def _store_iteration_data(self, run_id: str, iteration_number: int, model_responses: List[IterationResult], synthesis_result: ArbiterSynthesisResult) -> int:
        """Stores iteration details and links responses."""
        logger.debug(f"Storing data for Run ID {run_id}, Iteration {iteration_number}")
        
        # Insert iteration record
        iteration_insert_data = {
            "run_id": run_id,
            "iteration_number": iteration_number,
            "arbiter_response_id": synthesis_result.response_id, # Might be None if arbiter logging failed
            "arbiter_parsed_confidence": synthesis_result.confidence,
            "arbiter_parsed_needs_iteration": 1 if synthesis_result.needs_iteration else 0,
        }
        # Remove None values to rely on table defaults if arbiter failed
        iteration_insert_data = {k: v for k, v in iteration_insert_data.items() if v is not None}

        iteration_pk = self.db["consortium_iterations"].insert(
            iteration_insert_data, 
            pk="iteration_id" # Specify the PK for last_pk
        ).last_pk
        
        # Insert links for each model response
        response_links = []
        for resp in model_responses:
            if resp.response_id: # Only link if log_response was successful
                response_links.append({
                    "iteration_id": iteration_pk,
                    "response_id": resp.response_id
                })
            else:
                logger.warning(f"Run {run_id} Iter {iteration_number}: Skipping link for model {resp.model} instance {resp.instance} due to missing response_id.")
        
        if response_links:
            # Use ignore=True for robustness against potential race conditions or retries (though unlikely here)
            self.db["consortium_iteration_responses"].insert_all(response_links, pk=("iteration_id", "response_id"), ignore=True) 
            logger.debug(f"Run {run_id} Iter {iteration_number}: Stored {len(response_links)} model response links for iteration_id {iteration_pk}.")

        return iteration_pk # Return the generated iteration ID


    def _get_model_responses(self, prompt_for_models: str, run_id: str, iteration_number: int) -> List[IterationResult]:
        """Gets responses from all configured models in parallel."""
        results: List[IterationResult] = []
        futures = []
        # Use ThreadPoolExecutor for I/O-bound tasks (API calls)
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor: # Limit workers slightly
            for model_name, count in self.config.models.items():
                for i in range(count):
                    instance_num = i + 1
                    # Submit task to executor
                    futures.append(
                        executor.submit(self._get_single_model_response, model_name, prompt_for_models, instance_num, run_id, iteration_number)
                    )
            
            # Collect results as they complete
            for future in concurrent.futures.as_completed(futures):
                try:
                    results.append(future.result())
                except Exception as exc:
                     # Log error from future if not caught inside the task
                     logger.error(f"Run {run_id} Iter {iteration_number}: Error collecting model response future: {exc}")
                     # Append an error result placeholder
                     results.append(IterationResult(model="Unknown", instance=0, error=str(exc)))

        logger.debug(f"Run {run_id} Iter {iteration_number}: Collected {len(results)} model responses.")
        return results

    def _get_single_model_response(self, model_name: str, prompt: str, instance_num: int, run_id: str, iteration_number: int) -> IterationResult:
        """Gets response from a single model instance, handles retries, logging."""
        log_prefix = f"Model {model_name} Inst {instance_num} (Run {run_id} Iter {iteration_number})"
        logger.debug(f"{log_prefix}: Getting response.")
        
        attempts = 0
        max_retries = 2 # Limit retries
        instance_key = f"{model_name}-{instance_num}" # Key for conversation tracking
        
        while attempts <= max_retries:
            try:
                # Construct prompt (simple XML wrapper used before, kept for consistency if models expect it)
                # Consider making this wrapper optional or configurable
                # xml_prompt = f"<prompt>
    <instruction>{prompt}</instruction>
</prompt>"
                xml_prompt = prompt # Use raw prompt for now

                conversation_id = self.conversation_ids.get(instance_key)
                
                # Get model and make API call
                model_obj = llm.get_model(model_name)
                response = model_obj.prompt(
                    xml_prompt,
                    conversation_id=conversation_id # Pass None if not found
                )
                
                # Update conversation ID if a new one was created
                new_conv_id = getattr(response, 'conversation_id', None)
                if new_conv_id and new_conv_id != conversation_id:
                    self.conversation_ids[instance_key] = new_conv_id
                    logger.debug(f"{log_prefix}: Updated conversation ID to {new_conv_id}")

                text = response.text()
                # Log the response and get its database ID
                response_db_id = log_response(response, f"{model_name}-inst{instance_num}") 

                # Check if logging failed
                if not response_db_id:
                     logger.error(f"{log_prefix}: Failed to log response to database.")
                     # Return error or partial result? Let's return partial for now.
                     # return IterationResult(model=model_name, instance=instance_num, error="Failed to log response")


                return IterationResult(
                    model=model_name,
                    instance=instance_num,
                    response_text=text,
                    confidence=self._extract_confidence(text), # Extract confidence from model's own response
                    conversation_id=new_conv_id,
                    response_id=response_db_id, # Store the DB ID
                    error=None
                )
            except Exception as e:
                attempts += 1
                logger.error(f"{log_prefix}: Attempt {attempts} failed: {e}")
                if "RateLimitError" in str(e) and attempts <= max_retries:
                    wait_time = 1.5 ** attempts # Slightly gentler backoff
                    logger.warning(f"{log_prefix}: Rate limit hit. Retrying in {wait_time:.1f}s...")
                    time.sleep(wait_time)
                elif attempts > max_retries:
                    logger.error(f"{log_prefix}: Max retries exceeded.")
                    return IterationResult(model=model_name, instance=instance_num, error=f"Failed after {max_retries + 1} attempts: {e}")
                else: # Non-retriable error or final attempt failed
                     logger.error(f"{log_prefix}: Unhandled error: {e}")
                     return IterationResult(model=model_name, instance=instance_num, error=str(e))
        
        # Fallback if loop structure has issues
        logger.error(f"{log_prefix}: Exited retry loop unexpectedly.")
        return IterationResult(model=model_name, instance=instance_num, error="Exited retry loop unexpectedly.")


    def _extract_confidence(self, text: str) -> Optional[float]:
        """Extracts confidence score (0-1) from text. Returns None if not found."""
        if not text: return None
        # Try XML tag first
        xml_match = re.search(r"<confidence>\s*(\d*\.?\d+)\s*</confidence>", text, re.IGNORECASE | re.DOTALL)
        if xml_match:
            try:
                value = float(xml_match.group(1).strip())
                logger.debug(f"Extracted confidence {value} via XML tag.")
                return value if 0 <= value <= 1 else None
            except ValueError: pass

        # Fallback: Look for "Confidence: x.y" or "Confidence: x%" case-insensitively
        text_match = re.search(r"confidence\s*[:=]?\s*(\d*\.?\d+)\s*%?", text, re.IGNORECASE)
        if text_match:
            try:
                value = float(text_match.group(1).strip())
                if value > 1 and value <= 100: # Treat as percentage if > 1 and <= 100
                     value /= 100.0 
                logger.debug(f"Extracted confidence {value} via text pattern.")
                return value if 0 <= value <= 1 else None
            except ValueError: pass
            
        # logger.debug("Could not extract confidence score from text.")
        return None # Return None if not found


    def _construct_iteration_prompt(self, original_prompt: str, last_synthesis: ArbiterSynthesisResult) -> str:
        """Constructs the prompt for the *models* in the next iteration."""
        logger.debug("Constructing prompt for next model iteration.")
        if not ITERATION_PROMPT_TEMPLATE:
            logger.error("Iteration prompt template is missing!")
            return f"Please improve your previous response to the original prompt:
{original_prompt}" # Basic fallback

        # Prepare data for formatting
        user_instructions = self.config.system_prompt or ""
        # Use the arbiter's full result object (converted to dict) for previous_synthesis
        previous_synthesis_json = json.dumps(last_synthesis.model_dump(exclude_none=True), indent=2)
        # Format refinement areas cleanly
        refinement_areas_str = "
".join(f"- {area}" for area in last_synthesis.refinement_areas) if last_synthesis.refinement_areas else "No specific refinement areas provided."

        try:
            # Ensure all expected keys are present in the format call
            return ITERATION_PROMPT_TEMPLATE.format(
                original_prompt=original_prompt,
                previous_synthesis=previous_synthesis_json,
                user_instructions=user_instructions,
                refinement_areas=refinement_areas_str
            )
        except KeyError as e:
            logger.error(f"KeyError formatting iteration prompt template: Missing key '{e}'. Using fallback.")
            return f"Refining response for original prompt:
{original_prompt}

Arbiter feedback:
{previous_synthesis_json}

Please improve based on this feedback, especially the refinement areas."
        except Exception as format_err:
             logger.error(f"Unexpected error formatting iteration prompt: {format_err}. Using fallback.")
             return f"Refining response for original prompt:
{original_prompt}

Arbiter feedback:
{previous_synthesis_json}

Please improve based on this feedback."


    def _format_history_for_arbiter(self, history: List[Dict[str, Any]]) -> str:
        """Formats the iteration history for the arbiter prompt."""
        if not history: return "<no_previous_iterations />"
        
        formatted_history = []
        for iter_ctx in history:
            iter_num = iter_ctx.get('iteration_number', 'N/A')
            synth = iter_ctx.get('synthesis', {})
            model_resps = iter_ctx.get('model_responses', [])
            
            # Format model responses for this iteration
            model_resp_str = "
".join(
                 # Added check for response_text key
                 f"<model_response>
  <model>{r.get('model', 'Unknown')}</model>
  <instance>{r.get('instance', 1)}</instance>
  <response>{r.get('response_text', r.get('error', 'No response/Error'))}</response>
</model_response>"
                 for r in model_resps
            )
            # Format refinement areas for this iteration's synthesis
            ref_areas_str = "
".join(f"    <area>{area}</area>" for area in synth.get('refinement_areas', []))

            formatted_history.append(f"""<iteration iteration_number="{iter_num}">
  <model_responses>
{model_resp_str}
  </model_responses>
  <arbiter_synthesis>
    <synthesis>{synth.get('synthesis', 'N/A')}</synthesis>
    <confidence>{synth.get('confidence', 'N/A')}</confidence>
    <analysis>{synth.get('analysis', 'N/A')}</analysis>
    <dissent>{synth.get('dissent', 'N/A')}</dissent>
    <needs_iteration>{synth.get('needs_iteration', 'N/A')}</needs_iteration>
    <refinement_areas>
{ref_areas_str if ref_areas_str else '    No refinement areas specified.'}
    </refinement_areas>
  </arbiter_synthesis>
</iteration>""")
        return "
".join(formatted_history)


    def _format_responses_for_arbiter(self, responses: List[IterationResult]) -> str:
        """Formats the current model responses for the arbiter prompt."""
        formatted = []
        for r in responses:
            # Determine response content (text or error)
            response_content = r.response_text if r.response_text else f"Error: {r.error}"
            formatted.append(f"""<model_response>
  <model>{r.model}</model>
  <instance>{r.instance}</instance>
  <confidence>{r.confidence if r.confidence is not None else 'N/A'}</confidence>
  <response>{response_content}</response>
</model_response>""")
        return "
".join(formatted)

    def _synthesize_responses(self, original_prompt: str, current_model_responses: List[IterationResult], iteration_history: List[Dict[str, Any]], run_id: str, iteration_number: int) -> ArbiterSynthesisResult:
        """Calls the arbiter model and parses its response."""
        log_prefix = f"Arbiter ({self.arbiter_model_name}) (Run {run_id} Iter {iteration_number})"
        logger.debug(f"{log_prefix}: Synthesizing responses.")
        
        if not ARBITER_PROMPT_TEMPLATE:
             logger.error("Arbiter prompt template is missing!")
             # Return an ArbiterSynthesisResult indicating the error
             return ArbiterSynthesisResult(synthesis="Error: Arbiter prompt template missing.", error="Arbiter prompt template missing.", needs_iteration=False) 

        arbiter_model = llm.get_model(self.arbiter_model_name)

        # Prepare prompt context
        formatted_history = self._format_history_for_arbiter(iteration_history)
        formatted_responses = self._format_responses_for_arbiter(current_model_responses)
        user_instructions = self.config.system_prompt or ""

        # Format the arbiter prompt
        try:
            arbiter_prompt_text = ARBITER_PROMPT_TEMPLATE.format(
                original_prompt=original_prompt,
                formatted_responses=formatted_responses,
                formatted_history=formatted_history,
                user_instructions=user_instructions
            )
            logger.debug(f"{log_prefix}: Arbiter prompt ready.")
            # logger.debug(f"Arbiter Prompt:
{arbiter_prompt_text}") # Uncomment carefully for debugging prompts
        except KeyError as e:
            error_msg = f"Arbiter prompt formatting error: Missing key '{e}'"
            logger.error(error_msg)
            return ArbiterSynthesisResult(synthesis=f"Error: {error_msg}", error=error_msg, needs_iteration=False)
        except Exception as format_err:
            error_msg = f"Unexpected arbiter prompt formatting error: {format_err}"
            logger.error(error_msg)
            return ArbiterSynthesisResult(synthesis=f"Error: {error_msg}", error=error_msg, needs_iteration=False)

        # Call arbiter model
        try:
            logger.debug(f"{log_prefix}: Sending prompt to arbiter.")
            arbiter_llm_response = arbiter_model.prompt(arbiter_prompt_text)
            logger.debug(f"{log_prefix}: Received response from arbiter.")
            
            # Log arbiter response and get its DB ID
            arbiter_response_db_id = log_response(arbiter_llm_response, f"Arbiter-{self.arbiter_model_name}")
            if not arbiter_response_db_id:
                 logger.error(f"{log_prefix}: Failed to log arbiter response to database.")
                 # Proceed with parsing but response_id will be None

            # Parse the arbiter's text response
            parsed_result = self._parse_arbiter_response(arbiter_llm_response.text())
            # Add the database ID to the parsed result object
            parsed_result.response_id = arbiter_response_db_id
            logger.debug(f"{log_prefix}: Parsed arbiter response.")
            return parsed_result

        except Exception as e:
            logger.exception(f"{log_prefix}: Error during arbiter call or parsing: {e}")
            # Return an ArbiterSynthesisResult indicating the error
            return ArbiterSynthesisResult(synthesis=f"Error during arbiter interaction: {e}", error=str(e), needs_iteration=False)

    def _parse_arbiter_response(self, text: str) -> ArbiterSynthesisResult:
        """Parses the arbiter's XML-like response text."""
        logger.debug("Parsing arbiter response text.")
        # Use the Pydantic model for default values
        result = ArbiterSynthesisResult()

        # Regex patterns for each section
        patterns = {
            "synthesis": r"<synthesis>([sS]*?)</synthesis>",
            "confidence": r"<confidence>\s*(\d*\.?\d+)\s*</confidence>",
            "analysis": r"<analysis>([sS]*?)</analysis>",
            "dissent": r"<dissent>([sS]*?)</dissent>",
            "needs_iteration": r"<needs_iteration>\s*(true|false)\s*</needs_iteration>",
            "refinement_areas": r"<refinement_areas>([sS]*?)</refinement_areas>"
        }

        # Extract content for each field
        for key, pattern in patterns.items():
            match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
            if match:
                content = match.group(1).strip()
                try:
                    if key == "confidence":
                        result.confidence = float(content)
                    elif key == "needs_iteration":
                        result.needs_iteration = content.lower() == "true"
                    elif key == "refinement_areas":
                        # Try extracting <area> tags first
                        area_matches = re.findall(r"<area>([^<]+)</area>", content, re.IGNORECASE | re.DOTALL)
                        if area_matches:
                            result.refinement_areas = [area.strip() for area in area_matches if area.strip()]
                        elif content: # Fallback to lines if content exists but no <area> tags
                            result.refinement_areas = [area.strip().lstrip('- ') for area in content.split("
") if area.strip()]
                        # else: Keep default empty list if content is empty
                    else:
                        # Use setattr for synthesis, analysis, dissent
                        setattr(result, key, content)
                except (ValueError, TypeError) as parse_error:
                     logger.warning(f"Error parsing arbiter field '{key}' with content '{content}': {parse_error}")
                     # Keep default value for the field on error
            else:
                 # Only log warning if the tag is expected but missing (e.g., synthesis, confidence)
                 if key in ["synthesis", "confidence", "needs_iteration"]:
                      logger.warning(f"Could not find mandatory tag '{key}' in arbiter response.")
                 else: # For optional tags like analysis, dissent, refinement_areas
                      logger.debug(f"Optional tag '{key}' not found in arbiter response.")
        
        # logger.debug(f"Parsed arbiter response: {result.model_dump()}") # Log parsed result
        return result


# --- Saved Consortium Model ---

class ConsortiumModel(llm.Model):
    """Represents a saved consortium configuration callable via 'llm -m <name>'."""
    can_stream = False # Streaming is complex

    # Define runtime options that can override saved config values
    class Options(llm.Options):
        confidence_threshold: Optional[float] = None
        max_iterations: Optional[int] = None
        minimum_iterations: Optional[int] = None
        system_prompt: Optional[str] = None

    def __init__(self, model_id: str, config: ConsortiumConfig):
        self.model_id = model_id # This is the saved name
        self.saved_config = config # The config loaded from DB

    def __str__(self):
        return f"Consortium Model: {self.model_id}"

    def execute(self, prompt: llm.Prompt, stream, response: llm.Response, conversation):
        """Executes the consortium when called as a model."""
        run_id = str(uuid.uuid4()) # Unique ID for this execution
        logger.info(f"Executing saved consortium '{self.model_id}' as a model. Run ID: {run_id}")

        # 1. Combine Saved Config with Runtime Options
        current_config = self.saved_config.model_copy(deep=True)

        # Override with options from llm.Prompt.options (passed via -o key=value)
        # Check existence before accessing attributes
        options_dict = prompt.options.model_dump() if hasattr(prompt, 'options') and prompt.options else {}

        if options_dict.get('confidence_threshold') is not None:
            current_config.confidence_threshold = options_dict['confidence_threshold']
        if options_dict.get('max_iterations') is not None:
            current_config.max_iterations = options_dict['max_iterations']
        if options_dict.get('minimum_iterations') is not None:
            current_config.minimum_iterations = options_dict['minimum_iterations']
        if options_dict.get('system_prompt') is not None:
            current_config.system_prompt = options_dict['system_prompt']
        
        # System prompt from -s flag takes highest precedence
        if prompt.system:
            current_config.system_prompt = prompt.system
            
        logger.debug(f"Effective config for run {run_id}: {current_config.model_dump(exclude_none=True)}")

        # 2. Create Orchestrator and Run
        try:
            orchestrator = ConsortiumOrchestrator(current_config)
            # Pass the run_id explicitly to the orchestrate method
            result_payload = orchestrator.orchestrate(prompt.prompt, consortium_id=run_id) # Pass run_id as consortium_id

            # 3. Populate the main Response object for logging
            response.response_json = result_payload # Store full structured result
            # Add run_id attribute for logging clarity? The log_response needs response.id though.
            # The main response object's ID will be generated by log_response below.
            
            # 4. Log this *overall* consortium execution response
            # We pass self.model_id so the log shows the saved consortium name
            overall_response_id = log_response(response, self.model_id)
            logger.debug(f"Logged overall execution for saved consortium '{self.model_id}' with response ID {overall_response_id} (linked internally via Run ID {run_id})")

            # 5. Return the final text synthesis
            final_synthesis = result_payload.get("synthesis", {}).get("synthesis", "Consortium execution finished, but no final synthesis found.")
            return final_synthesis

        except Exception as e:
            logger.exception(f"Consortium execution failed for saved model '{self.model_id}' (Run ID: {run_id})")
            # Orchestrator should have marked run as failed in DB
            raise llm.ModelError(f"Consortium execution failed: {e}")


# --- Configuration Management ---

def _get_consortium_configs() -> Dict[str, ConsortiumConfig]:
    """Fetch saved consortium configurations."""
    try:
        db = DatabaseConnection.get_connection() # Ensures tables exist
        configs = {}
        if "consortium_configs" in db.table_names():
            for row in db["consortium_configs"].rows:
                try:
                    configs[row["name"]] = ConsortiumConfig.from_json(row["config"])
                except Exception as e:
                     logger.error(f"Error loading saved config '{row['name']}': {e}")
        else:
             logger.warning("Table 'consortium_configs' not found in database.")
        return configs
    except Exception as e:
        logger.error(f"Failed to load consortium configurations: {e}")
        return {}

def _save_consortium_config(name: str, config: ConsortiumConfig) -> None:
    """Save or update a consortium configuration."""
    db = DatabaseConnection.get_connection() # Ensures tables exist
    db["consortium_configs"].insert(
        {"name": name, "config": config.to_json()},
        pk="name",
        replace=True # Use replace=True to handle updates
    )
    logger.info(f"Saved consortium configuration '{name}'.")

# --- CLI Commands ---

from click_default_group import DefaultGroup
class DefaultToRunGroup(DefaultGroup):
     def __init__(self, *args, **kwargs):
        kwargs.setdefault('default_if_no_args', True)
        kwargs.setdefault('default', 'run')
        super().__init__(*args, **kwargs)

@llm.hookimpl
def register_commands(cli):
    @cli.group(cls=DefaultToRunGroup, default='run', default_if_no_args=True)
    def consortium():
        """Manage and run LLM Consortiums."""
        pass

    @consortium.command(name="run")
    @click.argument("prompt", required=False)
    @click.option(
        "-m", "--model", "models",
        multiple=True,
        help="Model & instance count (e.g., 'model-name:2'). Default: 1 instance.",
        default=["claude-3.5-sonnet:1"], # Example default
    )
    @click.option("--arbiter", help="Arbiter model.", default="claude-3.5-sonnet")
    @click.option("--confidence-threshold", type=click.FloatRange(0, 1), default=0.8)
    @click.option("--max-iterations", type=int, default=3)
    @click.option("--min-iterations", type=int, default=1)
    @click.option("--system", help="System prompt override.")
    @click.option("--output", type=click.Path(dir_okay=False, writable=True), help="Save full JSON results.")
    @click.option("--stdin/--no-stdin", default=True, help="Read extra input from stdin.")
    @click.option("--raw", is_flag=True, help="Output detailed execution trace.")
    def run_command(prompt, models, arbiter, confidence_threshold, max_iterations,
                   min_iterations, system, output, stdin, raw):
        """Run a prompt through a new consortium."""
        # Parse models:count syntax
        model_dict = {}
        for m_spec in models:
            parts = m_spec.split(':', 1)
            model_name = parts[0]
            count = 1
            if len(parts) == 2:
                try: 
                    count = int(parts[1])
                    if count < 1: raise ValueError("Count must be at least 1")
                except ValueError as e: raise click.ClickException(f"Invalid count in model spec '{m_spec}': {e}")
            model_dict[model_name] = count
        
        # Handle prompt and stdin
        cli_prompt = prompt # Store original CLI argument
        if not cli_prompt and stdin: 
            prompt_content = read_stdin_if_not_tty()
            if not prompt_content: raise click.UsageError("No prompt provided via argument or stdin.")
            final_prompt = prompt_content
        elif cli_prompt:
            final_prompt = cli_prompt
            if stdin: # Append stdin if CLI prompt was also given
                 stdin_content = read_stdin_if_not_tty()
                 if stdin_content:
                      final_prompt = f"{final_prompt}

{stdin_content}"
        else: # Should be caught by earlier check, but safeguard
             raise click.UsageError("Cannot determine prompt.")


        # Create config and orchestrator
        config = ConsortiumConfig(
            models=model_dict, arbiter=arbiter, system_prompt=system,
            confidence_threshold=confidence_threshold,
            max_iterations=max_iterations, minimum_iterations=min_iterations
        )
        orchestrator = ConsortiumOrchestrator(config)

        try:
            result_payload = orchestrator.orchestrate(final_prompt) # run_id generated inside
            
            if output:
                try:
                    with open(output, 'w') as f: json.dump(result_payload, f, indent=2)
                    logger.info(f"Full results saved to {output}")
                except Exception as e: logger.error(f"Failed to write output file: {e}")

            # Output final synthesis
            final_synthesis_text = result_payload.get("synthesis", {}).get("synthesis", "No synthesis found.")
            click.echo(final_synthesis_text)

            if raw: # Output detailed trace if requested
                click.echo("
--- Execution Trace ---")
                # Pretty print the JSON output
                click.echo(json.dumps(result_payload, indent=2)) 

        except Exception as e:
            logger.exception("Consortium run command failed.")
            raise click.ClickException(f"Execution failed: {e}")

    @consortium.command()
    @click.argument("name")
    @click.option("-m", "--model", "models", multiple=True, required=True, help="Model & count ('name:count').")
    @click.option("--arbiter", required=True)
    @click.option("--confidence-threshold", type=click.FloatRange(0, 1), default=0.8)
    @click.option("--max-iterations", type=int, default=3)
    @click.option("--min-iterations", type=int, default=1)
    @click.option("--system", help="System prompt to save.")
    def save(name, models, arbiter, confidence_threshold, max_iterations, min_iterations, system):
        """Save consortium settings."""
        model_dict = {}
        for m_spec in models:
             parts = m_spec.split(':', 1)
             model_name, count_str = (parts[0], parts[1]) if len(parts) == 2 else (parts[0], '1')
             try: 
                 count = int(count_str)
                 if count < 1: raise ValueError("Count must be >= 1")
                 model_dict[model_name] = count
             except ValueError as e: raise click.ClickException(f"Invalid count in model spec '{m_spec}': {e}")
        
        if not model_dict: raise click.ClickException("At least one model must be specified.")

        config = ConsortiumConfig(models=model_dict, arbiter=arbiter, system_prompt=system,
                                  confidence_threshold=confidence_threshold, max_iterations=max_iterations,
                                  minimum_iterations=min_iterations)
        try:
            _save_consortium_config(name, config)
            click.echo(f"Consortium '{name}' saved.")
        except Exception as e:
             raise click.ClickException(f"Failed to save: {e}")

    @consortium.command("list")
    def list_configs():
        """List saved consortiums."""
        configs = _get_consortium_configs()
        if not configs: click.echo("No saved consortiums."); return
        click.echo("Saved Consortiums:")
        for name, cfg in configs.items():
             # Format models string
             models_str = ", ".join([f"{m}:{c}" for m, c in cfg.models.items()])
             click.echo(f"- {name}: Arbiter={cfg.arbiter}, Models=[{models_str}], Confidence={cfg.confidence_threshold}, MaxIter={cfg.max_iterations}, MinIter={cfg.minimum_iterations}")

    @consortium.command()
    @click.argument("name")
    def remove(name):
        """Remove a saved consortium."""
        try:
            db = DatabaseConnection.get_connection()
            # Ensure table exists before deleting
            if "consortium_configs" not in db.table_names():
                 raise click.ClickException("Consortium config table does not exist.")
            
            deleted_count = db["consortium_configs"].delete_where("name = ?", [name])
            
            if deleted_count > 0: click.echo(f"Consortium '{name}' removed.")
            else: raise click.ClickException(f"Consortium '{name}' not found.")
        except Exception as e:
            logger.error(f"Failed to remove consortium '{name}': {e}")
            raise click.ClickException(f"Failed to remove: {e}")

@llm.hookimpl
def register_models(register):
    """Register saved consortiums as callable models."""
    logger.debug("Registering saved consortium configurations as models.")
    configs = _get_consortium_configs()
    for name, config in configs.items():
        try:
            register(ConsortiumModel(name, config), aliases=(name,))
            logger.debug(f"Registered saved consortium: {name}")
        except Exception as e:
            logger.error(f"Failed to register consortium model '{name}': {e}")

# --- Plugin Metadata ---
class LLMConsortiumPlugin:
    __version__ = "0.5.1" # Version bump for bug fix

logger.debug("llm_consortium module initialization complete.")

__all__ = [ # Define public components
    'ConsortiumOrchestrator', 'ConsortiumConfig', 'ConsortiumModel',
    'register_commands', 'register_models'
]
Exit Code: 0
