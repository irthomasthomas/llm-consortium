Stdout/Stderr: bing: Processing URL: https://arxiv.org/abs/2502.07352
bing: Retrieved content from https://arxiv.org/abs/2502.07352
bing: Converted content to text
bing: Processing URL: https://arxiv.org/abs/2403.08495
bing: Retrieved content from https://arxiv.org/abs/2403.08495
bing: Converted content to text
bing: Processing URL: https://arxiv.org/abs/2504.07385
bing: Retrieved content from https://arxiv.org/abs/2504.07385
bing: Converted content to text
bing: Processing URL: https://arxiv.org/abs/2405.11874
bing: Retrieved content from https://arxiv.org/abs/2405.11874
bing: Converted content to text
bing: Processing URL: https://arxiv.org/abs/2310.19736
bing: Retrieved content from https://arxiv.org/abs/2310.19736
bing: Converted content to text
bing: Processing URL: https://arxiv.org/abs/2503.05142
bing: Retrieved content from https://arxiv.org/abs/2503.05142
bing: Converted content to text
bing: Processing URL: https://github.com/IAAR-Shanghai/xFinder
bing: Retrieved content from https://github.com/IAAR-Shanghai/xFinder
bing: Converted content to text
URL: https://arxiv.org/abs/2502.07352
Relevant Quotes:
This arXiv page presents a paper titled "Bridging the Evaluation Gap: Leveraging Large Language Models for Topic Model Evaluation" by Zhiyin Tan and Jennifer D'Souza. The paper introduces a framework for automated evaluation of topic taxonomies using Large Language Models (LLMs), measuring qualities like coherence and diversity without relying on human annotators.

There are no passages or code related to automated evaluation techniques for large language models themselves. The paper focuses on using LLMs to evaluate topic models, which are a different kind of model.
--------------------------------------------------

URL: https://arxiv.org/abs/2403.08495
Relevant Quotes:
This arXiv page presents a research paper titled "Automatic Interactive Evaluation for Large Language Models with State Aware Patient Simulator." The paper introduces an Automated Interactive Evaluation (AIE) framework and the State-Aware Patient Simulator (SAPS) for assessing LLMs in clinical tasks through multi-turn doctor-patient simulations.

> In the quest to enhance the application of Large Language Models (LLMs) in healthcare, this paper introduces the Automated Interactive Evaluation (AIE) framework and the State-Aware Patient Simulator (SAPS), targeting the gap between traditional LLM evaluations and the nuanced demands of clinical practice. Unlike prior methods that rely on static medical knowledge assessments, AIE and SAPS provide a dynamic, realistic platform for assessing LLMs through multi-turn doctor-patient simulations. This approach offers a closer approximation to real clinical scenarios and allows for a detailed analysis of LLM behaviors in response to complex patient interactions. Our extensive experimental validation demonstrates the effectiveness of the AIE framework, with outcomes that align well with human evaluations, underscoring its potential to revolutionize medical LLM testing for improved healthcare delivery.
--------------------------------------------------

URL: https://arxiv.org/abs/2504.07385
Relevant Quotes:
This arXiv page displays a computer science paper titled "TALE: A Tool-Augmented Framework for Reference-Free Evaluation of Large Language Models" by Sher Badshah and others, submitted on April 10, 2025. The paper introduces a tool-augmented framework, TALE, designed to evaluate large language models without relying on pre-annotated references, using tool access to retrieve and synthesize external evidence, and it provides links to full text, references, citations, and related resources.

> Abstract:As Large Language Models (LLMs) become increasingly integrated into real-world, autonomous applications, relying on static, pre-annotated references for evaluation poses significant challenges in cost, scalability, and completeness. We propose Tool-Augmented LLM Evaluation (TALE), a framework to assess LLM outputs without predetermined ground-truth answers. Unlike conventional metrics that compare to fixed references or depend solely on LLM-as-a-judge knowledge, TALE employs an agent with tool-access capabilities that actively retrieves and synthesizes external evidence. It iteratively generates web queries, collects information, summarizes findings, and refines subsequent searches through reflection. By shifting away from static references, TALE aligns with free-form question-answering tasks common in real-world scenarios. Experimental results on multiple free-form QA benchmarks show that TALE not only outperforms standard reference-based metrics for measuring response accuracy but also achieves substantial to near-perfect agreement with human evaluations. TALE enhances the reliability of LLM evaluations in real-world, dynamic scenarios without relying on static references.
--------------------------------------------------

URL: https://arxiv.org/abs/2405.11874
Relevant Quotes:
This is an arXiv page for the paper "xFinder: Large Language Models as Automated Evaluators for Reliable Evaluation" by Qingchen Yu et al., submitted on May 20, 2024, and last revised on February 25, 2025. The paper proposes a novel evaluator, xFinder, for answer extraction and matching in LLM evaluation, using a specialized dataset called the Key Answer Finder (KAF) dataset to improve extraction accuracy and evaluation reliability, outperforming existing evaluation frameworks and judge models.

> The continuous advancement of large language models (LLMs) has brought increasing attention to the critical issue of developing fair and reliable methods for evaluating their performance.

> recent studies proposing fine-tuned LLMs as judge models for automated evaluation face challenges in terms of generalization ability and fairness.

> This paper comprehensively analyzes the entire LLM evaluation chain and demonstrates that optimizing the key answer extraction module improves extraction accuracy and enhances evaluation reliability. Our findings suggest that improving the key answer extraction module can lead to higher judgment accuracy and improved evaluation efficiency compared to the judge models.

> To address these issues, we propose xFinder, a novel evaluator for answer extraction and matching in LLM evaluation.
--------------------------------------------------

URL: https://arxiv.org/abs/2310.19736
Relevant Quotes:
This arXiv page presents a research paper titled "Evaluating Large Language Models: A Comprehensive Survey" by Zishan Guo et al., submitted on October 30, 2023, and last revised on November 25, 2023. The paper categorizes LLM evaluation into knowledge/capability, alignment, and safety, providing a comprehensive review of methodologies, benchmarks, and performance in specialized domains, aiming to guide responsible development by maximizing societal benefit and minimizing risks.

**Relevant Passages and CODE:**

*   Abstract: "To effectively capitalize on LLM capacities as well as ensure their safe and beneficial development, it is critical to conduct a rigorous and comprehensive evaluation of LLMs. This survey endeavors to offer a panoramic perspective on the evaluation of LLMs. We categorize the evaluation of LLMs into three major groups: knowledge and capability evaluation, alignment evaluation and safety evaluation. In addition to the comprehensive review on the evaluation methodologies and benchmarks on these three aspects, we collate a compendium of evaluations pertaining to LLMs' performance in specialized domains, and discuss the construction of comprehensive evaluation platforms that cover LLM evaluations on capabilities, alignment, safety, and applicability. We hope that this comprehensive overview will stimulate further research interests in the evaluation of LLMs, with the ultimate goal of making evaluation serve as a cornerstone in guiding the responsible development of LLMs."
--------------------------------------------------

URL: https://arxiv.org/abs/2503.05142
Relevant Quotes:
This page presents a research paper titled "RocketEval: Efficient Automated LLM Evaluation via Grading Checklist" which introduces a new automated evaluation method for large language models (LLMs) using a lightweight LLM judge. The method reframes the evaluation task as a multi-faceted Q&A with a checklist, addressing uncertainty and positional bias, and demonstrates high correlation with human preferences at a significantly reduced cost.

**Relevant Passages and Code:**

*   "Evaluating large language models (LLMs) in diverse and challenging scenarios is essential to align them with human preferences."
*   "To mitigate the prohibitive costs associated with human evaluations, utilizing a powerful LLM as a judge has emerged as a favored approach."
*   "In this paper, we propose a straightforward, replicable, and accurate automated evaluation method by leveraging a lightweight LLM as the judge, named RocketEval."
*   "By reframing the evaluation task as a multi-faceted Q&A using an instance-specific checklist, we demonstrate that the limited judgment accuracy of lightweight LLMs is largely attributes to high uncertainty and positional bias."
*   "To address these challenges, we introduce an automated evaluation process grounded in checklist grading, which is designed to accommodate a variety of scenarios and questions. This process encompasses the creation of checklists, the grading of these checklists by lightweight LLMs, and the reweighting of checklist items to align with the supervised annotations."
*   "Our experiments carried out on the automated evaluation benchmarks, MT-Bench and WildBench datasets, reveal that RocketEval, when using Gemma-2-2B as the judge, achieves a high correlation (0.965) with human preferences, which is comparable to GPT-4o. Moreover, RocketEval provides a cost reduction exceeding 50-fold for large-scale evaluation and comparison scenarios."
--------------------------------------------------

URL: https://github.com/IAAR-Shanghai/xFinder
Relevant Quotes:
xFinder is a novel evaluator that uses large language models for accurate answer extraction in LLM evaluations, addressing issues with RegEx methods and improving overall evaluation reliability. The project provides code and datasets for training and using xFinder, demonstrating its superior performance compared to existing frameworks in terms of extraction and judgment accuracy.

*   Abstract The continuous advancement of large language models (LLMs) has brought increasing attention to the critical issue of developing fair and reliable methods for evaluating their performance.
*   This paper comprehensively analyzes the entire LLM evaluation chain and demonstrates that optimizing the key answer extraction module improves extraction accuracy and enhances evaluation reliability.
*   Our findings suggest that improving the key answer extraction module can lead to higher judgment accuracy and improved evaluation efficiency compared to the judge models. To address these issues, we propose xFinder, a novel evaluator for answer extraction and matching in LLM evaluation.
*   Generalization tests and real-world evaluations show that the smallest xFinder model, with only 500 million parameters, achieves an average extraction accuracy of 93.42%. In contrast, RegEx accuracy in the best evaluation framework is 74.38%. The final judgment accuracy of xFinder reaches 97.61%, outperforming existing evaluation frameworks and judge models. 
*   We provide a comprehensive review of LLM evaluation processes in the industry, identifying critical factors that can lead to unreliable evaluation results.
*   We introduce xFinder, a novel evaluator for answer extraction and matching in LLM evaluation, and construct the KAF dataset to support its training and testing.
*   Our experiments show that both RegEx-based frameworks and automated judge models exhibit unreliability, leading to low judgment accuracy. In contrast, our method achieves higher accuracy and improved evaluation efficiency.
*   To streamline the evaluation process using xFinder, we have standardized various mainstream benchmark datasets into a unified JSON format.
*   If you wish to evaluate your own datasets using xFinder, please refer to our provided script template benchmark_dataset_template.py for format conversion guidance.
*   # Perform an evaluation with xFinder (a built-in example)
CUDA_VISIBLE_DEVICES=0 python -m xfinder.eval --run-example --model-name xFinder-qwen1505 --inference-mode local --model-path-or-url /path/to/anonymized/model/xFinder-qwen1505
*   📚 Batch Evaluation of Summarized Experimental Results 
This method allows you to evaluate multiple examples stored in a JSON file.
*   📄 Single-Instance Evaluation Mode 
This method allows you to evaluate individual examples, which can be integrated into a LLM evaluation framework.
*   We demonstrate instances across four types of questions where RegEx fails to extract or frequently extracts incorrect answers, whereas xFinder accurately extracts the key answers.
*   Baseline: OpenCompass, LM Eval Harness, UltraEval, GPT-4. Our Method: xFinder-qwen1505, xFinder-qwen1518, xFinder-gemma7, xFinder-chatglm36base, xFinder-llama38, xFinder-llama38it.
--------------------------------------------------
Summary:
Exit Code: 0
