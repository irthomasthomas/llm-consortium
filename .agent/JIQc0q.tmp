[
  {
    "id": "01jrnem10x4drmyx21fdcc36d2",
    "model": "gemini-2.5-pro-exp-03-25",
    "prompt": "Neither <COMMAND>, <WRITE_FILES>, <REASONING>, <GET_FULL>, <MEMORY> or <FINAL_ANSWER> received. Please provide one of these actions or reasoning.",
    "system": "You are DEEPBLOOM, a highly capable AI agent running in a Zsh shell environment on Linux. Your primary goal is to assist the user by executing commands, managing files, reasoning through problems, and providing accurate final answers.\n\n**Interaction Flow:**\n1.  **User Request:** You receive a request from the user, potentially enhanced with context (files, search results, memories).\n2.  **Reasoning & Planning:** Analyze the request. Use the `<REASONING>` tag to explain your thought process and the `<PLAN>` tag to outline the steps you will take.\n3.  **Action:** Choose ONE of the following actions per turn:\n    *   **`<COMMAND>command</COMMAND>`:** Execute a Zsh shell command. Assume standard Linux utilities are available (`ls`, `cd`, `grep`, `sed`, `awk`, `curl`, `python`, `git`, etc.). You will receive the `Stdout/Stderr` and `Exit Code`. Handle potential errors. Use `cat` for file content, but be mindful of large files (output might be truncated).\n    *   **`<WRITE_FILES>...</WRITE_FILES>`:** Write content to one or more files. Inside this block, use `<file path=\\\"/path/to/filename.ext\\\">File content</file>` to specify the file path and content.\n    *   **`<MEMORY>text to remember</MEMORY>`:** Store a piece of information in your long-term memory. Keep it concise.\n    *   **`<FINAL_ANSWER>answer</FINAL_ANSWER>`:** Provide the final answer to the user's request when you are confident the task is complete.\n4.  **Feedback:** You receive feedback based on your action (command output, file write status, memory confirmation).\n5.  **Iteration:** Repeat steps 2-4 based on the feedback until the task is complete. Use `<REASONING>` and `<PLAN>` on each turn to show how you are interpreting feedback and deciding the next step.\n\n**Core Capabilities:**\n\n1.  **Web Research:**\n    *   `bing_search -q \"query\" -n [1-50]`\n    *   `google_search -q \"query\" -n [1-50]`\n    *   `search_ddg \"query\"`\n    *   `search_terms_generator 'prompt, query or task description' -n=[1-9]` [--reasoning [1-9] # amount to think about the query before generating terms]\n\n2.  **System Interaction:**\n    *   `<COMMAND>command</COMMAND>` (Execute any shell command)\n    *   `<COMMAND>screenshot screen</COMMAND>` (Take a screenshot of the current screen)\n\n3.  **Memory Management:**\n    *   `add_memory \"information\"`\n    *   `memories`\n\n4.  **Data Collection:**\n    *   `fetch_web_page \"URL\"`\n    *   `prompt_the_user \"title\" \"question\"`\n\n5.  **Python Environment Management (uv):**\n    *   `uv init`, `uv add`, `uv venv`, `uv run`\n\n6.  **Progress Tracking (`/home/thomas/Projects/claude.sh/task_tracking/agent_controller.sh`):**\n    *   `note \"text\"`\n    *   `stage \"stagename\"`\n    *   `current`\n    *   `next`\n    *   `ask \"question\"`\n\n7.  **ShellLM Helper Functions (LLM Passthrough):**\n    *   Use these specialized functions via `<COMMAND>` for specific sub-tasks when direct execution or simple reasoning isn't enough. They wrap LLM calls, so you can pass standard `llm` options (e.g., `-m model_name`, `-o temperature=0.8`). Use them judiciously.\n    *   **When to Use:**\n        *   `structured_chain_of_thought \"problem\"`: Before complex planning, to break down the problem logically.\n        *   `task_plan_generator \"description\"`: Early in complex tasks to generate a high-level plan outline (often informed by `structured_chain_of_thought`).\n        *   `shelp \"task description\"`: When you need assistance formulating a specific shell command.\n        *   `commit_generator`: ONLY when specifically asked to generate a git commit message based on staged changes.\n        *   `brainstorm_generator \"topic\"`: When the user explicitly asks for brainstorming or idea generation.\n        *   `prompt_engineer \"prompt\"`: If you need to refine a complex prompt before using it (rarely needed for self-operation).\n        *   `search_engineer '<task/question>' --count=5 -m gemini-2`: Use **early** in research or problem-solving to generate diverse, effective search queries *before* using standard search commands (`bing_search`, etc.). Improves search quality and focuses effort.\n        *   `code_refactor [file_path]` (or pipe code): Use when asked to improve, analyze, or refactor existing code. Provides automated refactoring and expert analysis. Invoke via `<COMMAND>code_refactor path/to/file.ext --lang=language</COMMAND>` or `<COMMAND>cat code.ext | code_refactor --lang=language</COMMAND>`.\n    *   **Example:** `<COMMAND>shelp \"list all files larger than 10MB in /tmp\" -m claude-3.5-haiku</COMMAND>`\n    *   **Strategic Use:** Employ these tools appropriately to save time and minimize computational cost compared to achieving the same results through general reasoning or multiple steps.\n\n**File Handling:**\n\n*   **Always use `<WRITE_FILES>` for creating and *overwriting* files:**\n    *   Provide the *entire*, **raw** content of each file directly between the `<file path=\"...\">` and `</file>` tags.\n    *   **Do NOT** wrap the content in extra quotes or escape characters like `\\\\n` or `\\\\\"` for JSON. The content should appear exactly as you want it written to the file.\n    *   **Example:**\n        ```xml\n        <WRITE_FILES>\n        <file path=\"/path/to/script.py\">\n        import sys\n\n        print(\"Hello\\nWorld!\")\n        # Note: Raw newline above, not \\\\n\n        </file>\n        <file path=\"/path/to/config.json\">\n        {\n            \"key\": \"value\",\n            \"list\": [1, 2, 3],\n            \"nested\": {\n                \n                \"key\": \"value with \\\"quotes\\\"\"\n            }\n        }\n        # Note: Raw quotes above, not \\\\\"\n        </file>\n        <file path=\"/path/to/data.txt\">\n        Line 1\n        Line 2 with a tab       and a backslash \\\n        </file>\n        # Note: Raw tab and backslash above, not \\\\t or \\\\\\\\\n        </WRITE_FILES>\n        ```\n\n*   **For *modifying* existing files, use string replacement instead of rewriting the entire file whenever possible.** Use tools like `sed` within a `<COMMAND>` tag. Example:\n    `<COMMAND>sed -i 's/original_text/new_text/g' /path/to/file.txt</COMMAND>`\n\n*   Always use complete, absolute paths. The `<WRITE_FILES>` tag automatically creates directories as needed.\n\n    **String Replacement Guidelines**\n    1.  **Target Specificity**: Make your search patterns as specific as possible to avoid unintended changes.\n    2.  **Backup (When Necessary)**: If you're making extensive or uncertain changes, consider creating a backup of the file first within your plan, using the `cp` command within a `<COMMAND>` tag.\n    3.  **Complete Paths**: Always specify complete file paths.\n\n**System Interaction:**\n\n*   Avoid interactive applications (e.g., nano, vim). Use `<WRITE_FILES>` or string replacement.\n*   For user input, use: `<COMMAND>kdialog --inputbox \"Question\" \"Default Value\"</COMMAND>`\n*   Use the agent_controller.sh script to track your progress.\n\n**Memory:**\n\nUse `<MEMORY>` to store crucial information. Access stored memories with the `memories` command.\n\n**Key Guidelines:**\n*   **XML Tags:** Strictly adhere to the specified XML tags (`<COMMAND>`, `<WRITE_FILES>`, `<file>`, `<REASONING>`, `<PLAN>`, `<MEMORY>`, `<FINAL_ANSWER>`). Content goes *between* the opening and closing tags.\n*   **One Action Per Turn:** Only use *one* primary action tag (`<COMMAND>`, `<WRITE_FILES>`, `<MEMORY>`, `<FINAL_ANSWER>`) in each response. You can always include `<REASONING>` and `<PLAN>`.\n*   **Shell Environment:** You are in Zsh on Linux. Standard commands are available. Be careful with commands that might hang or require interaction. Use timeouts where appropriate if executing external scripts/tools.\n*   **File Paths:** Use absolute paths where possible, especially when writing files.\n*   **Error Handling:** If a command fails, analyze the error output and adjust your plan. Don't repeat failing commands without modification.\n*   **Conciseness:** Keep reasoning, plans, and memory entries concise.\n*   **Context:** Pay attention to provided context (`<FILE_CONTEXT>`, `<WEB_SEARCH_RESULTS>`, `<USER_RESPONSES>`, `<COMMAND_OUTPUTS>`, `<MEMORIES>`) and the feedback from your previous action.\n*   **Completion:** Only use `<FINAL_ANSWER>` when the user's request is fully addressed.\n\n**Workflow:**\n\n1.  Understand the request.\n2.  Plan your approach (`<PLAN>`).\n3.  Gather information (research, if needed).\n4.  Execute commands and review results.\n5.  Store important findings (`<MEMORY>`).\n6.  Present final solutions (`<FINAL_ANSWER>`).\n7.  When a task is complete use `<FINAL_ANSWER>` to provide, a final analysis identifying what went well, what challenges occurred, and lessons learned during the session.\n\nSEVERE WARNING: NEVER reference ANY of the above <tags> except when actively using them. You can write 'FINAL_ANSWER' in your response, but do not include the angle brackets unless you are using them as part of a command.\\n\\nIMPORTANT: You MUST use proper XML tags for your responses. Tags must be formatted as <TAG>content</TAG>, not as `<TAG>` or any other format. Required tags include <COMMAND>, <WRITE_FILES>, <REASONING>, <FINAL_ANSWER>, <PLAN>, and <MEMORY>.",
    "prompt_json": null,
    "options_json": {
      "stop": "</COMMAND>"
    },
    "response": "<REASONING>\nMy apologies for the previous incomplete response. I have now generated the complete, corrected code for `llm_consortium/__init__.py`.\n\nThis version implements the manual logging strategy:\n1.  A new function `manual_log_response` is introduced. It directly inserts essential data (model name, prompt, response text, duration, etc.) into the standard `responses` table using `sqlite-utils` and returns the generated row ID.\n2.  The core methods (`_get_single_model_response`, `_synthesize_responses`, `ConsortiumModel.execute`) have been modified to call `manual_log_response` instead of the old `log_response` (which has been removed).\n3.  The response ID returned by `manual_log_response` is captured and used to populate the foreign key columns in the new consortium-specific tables (`consortium_iterations`, `consortium_iteration_responses`), ensuring reliable linking between the core log and the consortium tracking data.\n4.  This bypasses potential issues with the base `llm` library's `log_to_db` method and gives us full control over the logging process within the consortium context.\n5.  The database schema creation logic (`ensure_consortium_tables`) remains to set up both the core `responses` table (with essential columns for manual logging) and the new consortium tables.\n\nThis approach directly addresses the problem of reliably linking consortium run details to the individual LLM responses logged in the database.\n</REASONING>\n\n<PLAN>\n1.  Retry the test command `llm consortium run \"What is 2+2?\" --model claude-3.5-sonnet:1 --arbiter claude-3.5-sonnet --max-iterations 1 --output /tmp/latest_result.json`.\n2.  If the command runs successfully, inspect the database:\n    *   Check the `responses` table for entries created by `manual_log_response`.\n    *   Check the `consortium_runs`, `consortium_iterations`, and `consortium_iteration_responses` tables to verify they are populated correctly with the linked `response_id`s.\n3.  If successful, proceed to the next major task: Designing and implementing the evaluation metric extraction.\n</PLAN>\n\n<WRITE_FILES>\n<file path=\"/home/thomas/Projects/llm/plugins/Utilities/llm-consortium/llm_consortium/__init__.py\">\nimport click\nimport json\nimport llm\nimport uuid\nfrom typing import Dict, List, Optional, Any, Tuple\nfrom datetime import datetime, timezone\nimport logging\nimport sys \nimport re\nimport os\nimport pathlib\nimport sqlite_utils\nfrom pydantic import BaseModel, Field\nimport time\nimport concurrent.futures\nimport threading\n\n# --- Database Setup ---\n\ndef user_dir() -> pathlib.Path:\n    path = pathlib.Path(click.get_app_dir(\"io.datasette.llm\"))\n    path.mkdir(parents=True, exist_ok=True)\n    return path\n\ndef logs_db_path() -> pathlib.Path:\n    return user_dir() / \"logs.db\"\n\n# Flag to track if tables have been ensured\n_tables_ensured = False\n_db_lock = threading.Lock() # Lock for ensuring tables safely\n\ndef ensure_consortium_tables(db: sqlite_utils.Database):\n    \"\"\"Create consortium-specific tables and ensure core 'responses' table exists.\"\"\"\n    global _tables_ensured\n    # Use a lock to prevent race conditions during table creation across threads\n    with _db_lock:\n        if _tables_ensured:\n            return\n\n        logger.debug(\"Ensuring database tables exist.\")\n        \n        # Ensure core 'responses' table exists with columns needed for manual insert\n        db[\"responses\"].create({\n            \"id\": str, # TEXT UUID PRIMARY KEY\n            \"model\": str,\n            \"prompt\": str,\n            \"system\": str, # Nullable\n            \"response\": str, # Nullable (e.g., if error occurred before response)\n            \"conversation_id\": str, # Nullable\n            \"datetime_utc\": str, # ISO 8601 TEXT\n            \"duration_ms\": int, # Nullable\n            \"response_json\": str, # Nullable JSON TEXT\n            # Add other columns if the core llm UI/library absolutely needs them,\n            # but keep it minimal for manual logging.\n            # \"prompt_json\": str, \n            # \"options_json\": str,\n            # \"input_tokens\": int,\n            # \"output_tokens\": int, \n        }, pk=\"id\", if_not_exists=True)\n\n        # Consortium specific tables\n        db[\"consortium_runs\"].create({\n            \"run_id\": str, # TEXT UUID\n            \"start_timestamp\": str, # ISO 8601 TEXT\n            \"end_timestamp\": str, # Nullable\n            \"status\": str, # 'running', 'completed', 'failed'\n            \"original_prompt\": str,\n            \"consortium_config\": str, # JSON TEXT\n            \"final_synthesis\": str, # Nullable\n            \"iterations_performed\": int, # Nullable\n            \"final_confidence\": float, # Nullable\n            \"error_message\": str, # Nullable\n        }, pk=\"run_id\", if_not_exists=True)\n\n        db[\"consortium_iterations\"].create({\n            \"iteration_id\": int, # INTEGER AUTOINCREMENT\n            \"run_id\": str, \n            \"iteration_number\": int,\n            \"arbiter_response_id\": str, # TEXT UUID from responses table, Nullable\n            \"arbiter_parsed_confidence\": float, # Nullable\n            \"arbiter_parsed_needs_iteration\": int, # Boolean (0/1), Nullable\n        }, pk=\"iteration_id\", foreign_keys=[\n            (\"run_id\", \"consortium_runs\", \"run_id\"),\n            (\"arbiter_response_id\", \"responses\", \"id\") \n        ], if_not_exists=True)\n        db[\"consortium_iterations\"].create_index([\"run_id\", \"iteration_number\"], unique=True, if_not_exists=True)\n\n        db[\"consortium_iteration_responses\"].create({\n            \"iteration_id\": int,\n            \"response_id\": str, # TEXT UUID from responses table\n        }, pk=(\"iteration_id\", \"response_id\"), foreign_keys=[\n            (\"iteration_id\", \"consortium_iterations\", \"iteration_id\"),\n            (\"response_id\", \"responses\", \"id\") \n        ], if_not_exists=True)\n        \n        db[\"consortium_configs\"].create({\n            \"name\": str, \"config\": str, \"created_at\": str\n        }, pk=\"name\", defaults={\"created_at\": \"CURRENT_TIMESTAMP\"}, if_not_exists=True)\n\n        _tables_ensured = True\n        logger.debug(\"Database tables ensured.\")\n\n\nclass DatabaseConnection:\n    _thread_local = threading.local()\n\n    @classmethod\n    def get_connection(cls) -> sqlite_utils.Database:\n        if not hasattr(cls._thread_local, 'db'):\n            db_path = logs_db_path()\n            try:\n                cls._thread_local.db = sqlite_utils.Database(db_path)\n                ensure_consortium_tables(cls._thread_local.db) # Ensure tables on first connection\n            except Exception as e:\n                logger.error(f\"Failed to connect to or setup database at {db_path}: {e}\")\n                raise\n        return cls._thread_local.db\n\n# --- Logging Setup ---\ndef setup_logging() -> None:\n    log_path = user_dir() / \"consortium.log\"\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    console_handler = logging.StreamHandler(sys.stderr); console_handler.setLevel(logging.ERROR); console_handler.setFormatter(formatter)\n    try:\n        file_handler = logging.FileHandler(str(log_path), mode='w'); file_handler.setLevel(logging.DEBUG); file_handler.setFormatter(formatter)\n    except Exception as e: print(f\"Error setting up file logging: {e}\", file=sys.stderr); file_handler = None\n    root_logger = logging.getLogger(); \n    if root_logger.hasHandlers(): root_logger.handlers.clear()\n    root_logger.setLevel(logging.DEBUG); root_logger.addHandler(console_handler)\n    if file_handler: root_logger.addHandler(file_handler)\n    logging.getLogger(\"llm_consortium\").setLevel(logging.DEBUG)\n\nsetup_logging()\nlogger = logging.getLogger(__name__)\nlogger.debug(\"llm_consortium module loaded, logging configured.\")\n\n# --- Utility Functions ---\n\ndef _read_prompt_file(filename: str) -> str:\n    try:\n        file_path = pathlib.Path(__file__).parent / filename\n        if not file_path.is_file(): logger.error(f\"Prompt file not found: {file_path}\"); return \"\"\n        with open(file_path, \"r\") as f: return f.read().strip()\n    except Exception as e: logger.error(f\"Error reading prompt file '{filename}': {e}\"); return \"\"\n\ndef read_stdin_if_not_tty() -> Optional[str]:\n    if not sys.stdin.isatty(): content = sys.stdin.read(); logger.debug(f\"Read {len(content)} bytes from stdin.\"); return content.strip()\n    logger.debug(\"Stdin is a TTY, not reading.\"); return None\n\nDEFAULT_SYSTEM_PROMPT = _read_prompt_file(\"system_prompt.txt\")\nARBITER_PROMPT_TEMPLATE = _read_prompt_file(\"arbiter_prompt.xml\")\nITERATION_PROMPT_TEMPLATE = _read_prompt_file(\"iteration_prompt.txt\")\nif not ARBITER_PROMPT_TEMPLATE: logger.error(\"CRITICAL: arbiter_prompt.xml is missing or empty!\")\nif not ITERATION_PROMPT_TEMPLATE: logger.error(\"CRITICAL: iteration_prompt.txt is missing or empty!\")\n\n\ndef manual_log_response(\n    model_name: str, \n    prompt_text: str, \n    response_text: str, \n    system_prompt: Optional[str] = None,\n    conversation_id: Optional[str] = None,\n    duration_ms: Optional[int] = None,\n    response_json: Optional[Any] = None # Accept dict or other json-serializable\n) -> Optional[str]:\n    \"\"\"Manually insert a record into the 'responses' table and return its ID.\"\"\"\n    try:\n        db = DatabaseConnection.get_connection()\n        response_id = str(uuid.uuid4())\n        record = {\n            \"id\": response_id,\n            \"model\": model_name,\n            \"prompt\": prompt_text,\n            \"system\": system_prompt,\n            \"response\": response_text,\n            \"conversation_id\": conversation_id,\n            \"datetime_utc\": datetime.now(timezone.utc).isoformat(),\n            \"duration_ms\": duration_ms,\n            # Ensure response_json is stored as a JSON string\n            \"response_json\": json.dumps(response_json) if response_json is not None else None, \n        }\n        # Remove keys with None values to avoid inserting NULLs explicitly unless needed\n        record_to_insert = {k: v for k, v in record.items() if v is not None} \n        \n        db[\"responses\"].insert(record_to_insert, pk=\"id\")\n        logger.debug(f\"Manually logged response for '{model_name}'. Response ID: {response_id}\")\n        return response_id\n    except Exception as e:\n        # Log the actual record attempted to be inserted for debugging\n        logger.exception(f\"Error manually logging response for {model_name}. Record data (approx): {record_to_insert.keys()}. Error: {e}\")\n        return None\n\n# --- Core Logic Classes ---\n\nclass IterationResult(BaseModel):\n    model: str\n    instance: int\n    response_text: Optional[str] = None\n    confidence: Optional[float] = None\n    conversation_id: Optional[str] = None\n    response_id: Optional[str] = None \n    prompt_used: Optional[str] = None \n    duration_ms: Optional[int] = None\n    error: Optional[str] = None\n\nclass ArbiterSynthesisResult(BaseModel):\n    synthesis: str = \"No synthesis found.\"\n    confidence: float = 0.0\n    analysis: str = \"No analysis found.\"\n    dissent: str = \"\"\n    needs_iteration: bool = True\n    refinement_areas: List[str] = Field(default_factory=list)\n    response_id: Optional[str] = None \n    prompt_used: Optional[str] = None \n    duration_ms: Optional[int] = None\n    response_json_raw: Optional[Any] = None # Store raw response JSON if available\n    error: Optional[str] = None \n\nclass ConsortiumConfig(BaseModel):\n    models: Dict[str, int] \n    system_prompt: Optional[str] = None\n    confidence_threshold: float = 0.8\n    max_iterations: int = 3\n    minimum_iterations: int = 1\n    arbiter: Optional[str] = None\n\n    def to_json(self) -> str: return self.model_dump_json()\n    @classmethod\n    def from_json(cls, json_str: str): return cls.model_validate_json(json_str)\n\nclass ConsortiumOrchestrator:\n    def __init__(self, config: ConsortiumConfig):\n        self.config = config\n        self.arbiter_model_name = config.arbiter or \"claude-3.5-sonnet\" \n        self.conversation_ids: Dict[str, Optional[str]] = {}\n        self.db = DatabaseConnection.get_connection() \n\n    def orchestrate(self, prompt: str) -> Dict[str, Any]:\n        run_id = str(uuid.uuid4())\n        start_time_iso = datetime.now(timezone.utc).isoformat()\n        start_time_perf = time.perf_counter()\n        logger.info(f\"Starting consortium run ID: {run_id}\")\n\n        self.db[\"consortium_runs\"].insert({\n            \"run_id\": run_id, \"start_timestamp\": start_time_iso, \"status\": \"running\",\n            \"original_prompt\": prompt, \"consortium_config\": self.config.to_json(),\n        }, pk=\"run_id\")\n\n        iteration_count = 0\n        final_synthesis_result = None\n        all_iteration_contexts = [] \n        current_prompt_for_models = prompt \n        if self.config.system_prompt: \n            current_prompt_for_models = f\"[SYSTEM INSTRUCTIONS]\\n{self.config.system_prompt}\\n[/SYSTEM INSTRUCTIONS]\\n\\n{prompt}\"\n\n        try:\n            while True: \n                iteration_count += 1\n                logger.debug(f\"Starting Iteration {iteration_count} for Run ID: {run_id}\")\n                prompt_this_iter = current_prompt_for_models \n\n                model_responses: List[IterationResult] = self._get_model_responses(prompt_this_iter, run_id, iteration_count)\n                synthesis_result: ArbiterSynthesisResult = self._synthesize_responses(prompt, model_responses, all_iteration_contexts, run_id, iteration_count)\n                iteration_db_id = self._store_iteration_data(run_id, iteration_count, model_responses, synthesis_result)\n                \n                all_iteration_contexts.append({\n                    \"iteration_number\": iteration_count,\n                    \"model_responses\": [r.model_dump(exclude_none=True) for r in model_responses], \n                    \"synthesis\": synthesis_result.model_dump(exclude_none=True)\n                })\n\n                met_min_iter = iteration_count >= self.config.minimum_iterations\n                met_confidence = synthesis_result.confidence >= self.config.confidence_threshold\n                arbiter_wants_stop = not synthesis_result.needs_iteration\n                \n                if met_min_iter and met_confidence and arbiter_wants_stop:\n                    logger.info(f\"Run {run_id} completed: Met criteria at iteration {iteration_count}.\")\n                    final_synthesis_result = synthesis_result; break\n                \n                if iteration_count >= self.config.max_iterations:\n                    logger.info(f\"Run {run_id} completed: Reached max iterations ({self.config.max_iterations}).\")\n                    final_synthesis_result = synthesis_result; break\n\n                logger.debug(f\"Run {run_id} preparing for iteration {iteration_count + 1}.\")\n                current_prompt_for_models = self._construct_iteration_prompt(prompt, synthesis_result) \n\n        except Exception as e:\n            logger.exception(f\"Error during orchestration for run {run_id}\")\n            self.db[\"consortium_runs\"].update(run_id, {\"status\": \"failed\", \"error_message\": str(e), \"end_timestamp\": datetime.now(timezone.utc).isoformat()}, alter=True) \n            raise llm.ModelError(f\"Consortium run {run_id} failed: {e}\") from e\n\n        end_time_iso = datetime.now(timezone.utc).isoformat()\n        total_duration_ms = int((time.perf_counter() - start_time_perf) * 1000)\n        final_synth_text = final_synthesis_result.synthesis if final_synthesis_result else \"N/A\"\n        final_conf = final_synthesis_result.confidence if final_synthesis_result else None\n\n        self.db[\"consortium_runs\"].update(run_id, {\n            \"status\": \"completed\", \"end_timestamp\": end_time_iso, \"iterations_performed\": iteration_count,\n            \"final_synthesis\": final_synth_text, \"final_confidence\": final_conf,\n        }, alter=True)\n        \n        logger.info(f\"Consortium run {run_id} finished successfully in {total_duration_ms}ms.\")\n\n        return {\n            \"original_prompt\": prompt,\n            \"synthesis\": final_synthesis_result.model_dump(exclude_none=True) if final_synthesis_result else {},\n            \"iteration_history\": all_iteration_contexts,\n            \"metadata\": {\n                \"run_id\": run_id, \"models_used\": self.config.models, \"arbiter\": self.arbiter_model_name,\n                \"start_timestamp\": start_time_iso, \"end_timestamp\": end_time_iso, \"total_time_ms\": total_duration_ms,\n                \"iteration_count\": iteration_count, \"confidence_threshold\": self.config.confidence_threshold,\n                \"max_iterations\": self.config.max_iterations, \"min_iterations\": self.config.minimum_iterations\n            }\n        }\n\n    def _store_iteration_data(self, run_id: str, iteration_number: int, model_responses: List[IterationResult], synthesis_result: ArbiterSynthesisResult) -> int:\n        logger.debug(f\"Storing data for Run ID {run_id}, Iteration {iteration_number}\")\n        iteration_insert_data = {\n            \"run_id\": run_id, \"iteration_number\": iteration_number, \"arbiter_response_id\": synthesis_result.response_id, \n            \"arbiter_parsed_confidence\": synthesis_result.confidence, \"arbiter_parsed_needs_iteration\": 1 if synthesis_result.needs_iteration else 0,\n        }\n        iteration_insert_data = {k: v for k, v in iteration_insert_data.items() if v is not None}\n        try:\n             iteration_pk = self.db[\"consortium_iterations\"].insert(iteration_insert_data, pk=\"iteration_id\").last_pk\n        except Exception as e: logger.error(f\"Failed to insert iteration record for Run {run_id} Iter {iteration_number}: {e}\"); return -1 \n\n        response_links = [{\"iteration_id\": iteration_pk, \"response_id\": resp.response_id} for resp in model_responses if resp.response_id]\n        if response_links:\n            try:\n                 self.db[\"consortium_iteration_responses\"].insert_all(response_links, pk=(\"iteration_id\", \"response_id\"), ignore=True) \n                 logger.debug(f\"Run {run_id} Iter {iteration_number}: Stored {len(response_links)} links for iter_id {iteration_pk}.\")\n            except Exception as e: logger.error(f\"Failed to insert response links for Iter ID {iteration_pk}: {e}\")\n        else: logger.warning(f\"Run {run_id} Iter {iteration_number}: No valid response IDs found to link.\")\n        return iteration_pk \n\n    def _get_model_responses(self, prompt_for_models: str, run_id: str, iteration_number: int) -> List[IterationResult]:\n        results: List[IterationResult] = []\n        futures = []\n        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor: \n            for model_name, count in self.config.models.items():\n                for i in range(count):\n                    instance_num = i + 1\n                    futures.append(executor.submit(self._get_single_model_response, model_name, prompt_for_models, instance_num, run_id, iteration_number))\n            for future in concurrent.futures.as_completed(futures):\n                try: results.append(future.result())\n                except Exception as exc: logger.error(f\"Run {run_id} Iter {iteration_number}: Error collecting future: {exc}\"); results.append(IterationResult(model=\"Unknown\", instance=0, error=str(exc)))\n        logger.debug(f\"Run {run_id} Iter {iteration_number}: Collected {len(results)} model responses.\")\n        return results\n\n    def _get_single_model_response(self, model_name: str, prompt: str, instance_num: int, run_id: str, iteration_number: int) -> IterationResult:\n        log_prefix = f\"Model {model_name} Inst {instance_num} (Run {run_id} Iter {iteration_number})\"\n        logger.debug(f\"{log_prefix}: Getting response.\")\n        attempts = 0; max_retries = 2; instance_key = f\"{model_name}-{instance_num}\"; start_time = time.perf_counter()\n        \n        while attempts <= max_retries:\n            duration_ms = int((time.perf_counter() - start_time) * 1000) # Capture duration inside loop\n            try:\n                conversation_id = self.conversation_ids.get(instance_key)\n                kwargs_for_prompt = {}\n                if conversation_id is not None: kwargs_for_prompt[\"conversation_id\"] = conversation_id\n                    \n                model_obj = llm.get_model(model_name)\n                api_response = model_obj.prompt(prompt, **kwargs_for_prompt) \n                duration_ms = int((time.perf_counter() - start_time) * 1000) # Update duration after call\n                \n                new_conv_id = getattr(api_response, 'conversation_id', None)\n                if new_conv_id and new_conv_id != conversation_id:\n                    self.conversation_ids[instance_key] = new_conv_id; logger.debug(f\"{log_prefix}: Updated conv ID to {new_conv_id}\")\n\n                text = api_response.text()\n                raw_response_json = getattr(api_response, 'response_json', None) # Get raw JSON if possible\n\n                response_db_id = manual_log_response(\n                    model_name=f\"{model_name}-inst{instance_num}\", prompt_text=prompt, response_text=text,\n                    system_prompt=self.config.system_prompt if iteration_number == 1 else None, \n                    conversation_id=new_conv_id, duration_ms=duration_ms, response_json=raw_response_json \n                )\n                if not response_db_id: logger.error(f\"{log_prefix}: Failed manual log insert.\")\n\n                return IterationResult(\n                    model=model_name, instance=instance_num, response_text=text,\n                    confidence=self._extract_confidence(text), conversation_id=new_conv_id,\n                    response_id=response_db_id, prompt_used=prompt, duration_ms=duration_ms, error=None\n                )\n            except Exception as e:\n                attempts += 1; logger.error(f\"{log_prefix}: Attempt {attempts} failed after {duration_ms}ms: {e}\")\n                if \"RateLimitError\" in str(e) and attempts <= max_retries:\n                    wait_time = 1.5 ** attempts; logger.warning(f\"{log_prefix}: Rate limit. Retrying in {wait_time:.1f}s...\"); time.sleep(wait_time)\n                elif attempts > max_retries: return IterationResult(model=model_name, instance=instance_num, error=f\"Failed after {max_retries + 1} attempts: {e}\", duration_ms=duration_ms, prompt_used=prompt)\n                elif attempts <= max_retries: logger.warning(f\"{log_prefix}: Non-rate-limit error, retrying...\"); time.sleep(1) \n                else: return IterationResult(model=model_name, instance=instance_num, error=str(e), duration_ms=duration_ms, prompt_used=prompt)\n        \n        logger.error(f\"{log_prefix}: Exited retry loop unexpectedly.\"); duration_ms = int((time.perf_counter() - start_time) * 1000)\n        return IterationResult(model=model_name, instance=instance_num, error=\"Exited retry loop unexpectedly.\", duration_ms=duration_ms, prompt_used=prompt)\n\n\n    def _extract_confidence(self, text: str) -> Optional[float]:\n        if not text: return None\n        xml_match = re.search(r\"<confidence>\\s*(\\d*\\.?\\d+)\\s*</confidence>\", text, re.IGNORECASE | re.DOTALL)\n        if xml_match:\n            try: value = float(xml_match.group(1).strip()); return value if 0 <= value <= 1 else None\n            except ValueError: pass\n        text_match = re.search(r\"confidence\\s*[:=]?\\s*(\\d*\\.?\\d+)\\s*%?\", text, re.IGNORECASE)\n        if text_match:\n            try:\n                value = float(text_match.group(1).strip()); value = value / 100.0 if value > 1 and value <= 100 else value\n                return value if 0 <= value <= 1 else None\n            except ValueError: pass\n        return None \n\n    def _construct_iteration_prompt(self, original_prompt: str, last_synthesis: ArbiterSynthesisResult) -> str:\n        logger.debug(\"Constructing prompt for next model iteration.\")\n        if not ITERATION_PROMPT_TEMPLATE: logger.error(\"Iteration prompt template missing!\"); return f\"Improve previous response to:\\n{original_prompt}\" \n        user_instructions = self.config.system_prompt or \"\"\n        previous_synthesis_json = json.dumps(last_synthesis.model_dump(exclude_none=True), indent=2)\n        refinement_areas_str = \"\\n\".join(f\"- {area}\" for area in last_synthesis.refinement_areas) or \"No specific refinement areas.\"\n        try:\n            return ITERATION_PROMPT_TEMPLATE.format(original_prompt=original_prompt, previous_synthesis=previous_synthesis_json, user_instructions=user_instructions, refinement_areas=refinement_areas_str)\n        except Exception as e: logger.error(f\"Error formatting iteration prompt: {e}. Using fallback.\"); return f\"Refine response for:\\n{original_prompt}\\nFeedback:\\n{previous_synthesis_json}\"\n\n    def _format_history_for_arbiter(self, history: List[Dict[str, Any]]) -> str:\n        if not history: return \"<no_previous_iterations />\"\n        formatted_history = []\n        for iter_ctx in history:\n            iter_num = iter_ctx.get('iteration_number', 'N/A'); synth = iter_ctx.get('synthesis', {}); model_resps = iter_ctx.get('model_responses', [])\n            model_resp_str = \"\\n\".join(f\"<resp model=\\\"{r.get('model', '?')}\\\" instance=\\\"{r.get('instance', 1)}\\\">{r.get('response_text', r.get('error', 'Error'))}</resp>\" for r in model_resps)\n            ref_areas_str = \"\\n\".join(f\"<area>{area}</area>\" for area in synth.get('refinement_areas', [])) or \"No refinement areas.\"\n            formatted_history.append(f\"<iteration num=\\\"{iter_num}\\\">\\n<model_responses>{model_resp_str}</model_responses>\\n<arbiter_feedback><confidence>{synth.get('confidence', 'N/A')}</confidence><needs_iter>{synth.get('needs_iteration', 'N/A')}</needs_iter><refinement>{ref_areas_str}</refinement></arbiter_feedback>\\n</iteration>\")\n        return \"\\n\".join(formatted_history)\n\n    def _format_responses_for_arbiter(self, responses: List[IterationResult]) -> str:\n        formatted = []\n        for r in responses:\n            response_content = r.response_text if r.response_text else f\"Error: {r.error}\"\n            formatted.append(f\"<model_response>\\n <model>{r.model}</model>\\n <instance>{r.instance}</instance>\\n <confidence>{r.confidence if r.confidence is not None else 'N/A'}</confidence>\\n <response>{response_content}</response>\\n</model_response>\")\n        return \"\\n\".join(formatted)\n\n    def _synthesize_responses(self, original_prompt: str, current_model_responses: List[IterationResult], iteration_history: List[Dict[str, Any]], run_id: str, iteration_number: int) -> ArbiterSynthesisResult:\n        log_prefix = f\"Arbiter ({self.arbiter_model_name}) (Run {run_id} Iter {iteration_number})\"; logger.debug(f\"{log_prefix}: Synthesizing.\")\n        start_time = time.perf_counter()\n        if not ARBITER_PROMPT_TEMPLATE: logger.error(\"Arbiter prompt template missing!\"); return ArbiterSynthesisResult(error=\"Arbiter template missing.\", needs_iteration=False) \n        arbiter_model = llm.get_model(self.arbiter_model_name)\n        formatted_history = self._format_history_for_arbiter(iteration_history)\n        formatted_responses = self._format_responses_for_arbiter(current_model_responses)\n        user_instructions = self.config.system_prompt or \"\"\n        try:\n            arbiter_prompt_text = ARBITER_PROMPT_TEMPLATE.format(original_prompt=original_prompt, formatted_responses=formatted_responses, formatted_history=formatted_history, user_instructions=user_instructions)\n            logger.debug(f\"{log_prefix}: Arbiter prompt ready.\")\n        except Exception as e: error_msg = f\"Arbiter prompt format error: {e}\"; logger.error(error_msg); return ArbiterSynthesisResult(error=error_msg, needs_iteration=False)\n\n        try:\n            logger.debug(f\"{log_prefix}: Sending prompt to arbiter.\")\n            api_response = arbiter_model.prompt(arbiter_prompt_text)\n            duration_ms = int((time.perf_counter() - start_time) * 1000); logger.debug(f\"{log_prefix}: Received response in {duration_ms}ms.\")\n            arbiter_response_text = api_response.text()\n            raw_response_json = getattr(api_response, 'response_json', None)\n            arbiter_response_db_id = manual_log_response(model_name=f\"Arbiter-{self.arbiter_model_name}\", prompt_text=arbiter_prompt_text, response_text=arbiter_response_text, duration_ms=duration_ms, response_json=raw_response_json)\n            if not arbiter_response_db_id: logger.error(f\"{log_prefix}: Failed manual log insert for arbiter.\")\n\n            parsed_result = self._parse_arbiter_response(arbiter_response_text)\n            parsed_result.response_id = arbiter_response_db_id; parsed_result.prompt_used = arbiter_prompt_text; parsed_result.duration_ms = duration_ms; parsed_result.response_json_raw = raw_response_json\n            logger.debug(f\"{log_prefix}: Parsed arbiter response.\")\n            return parsed_result\n        except Exception as e:\n            duration_ms = int((time.perf_counter() - start_time) * 1000); logger.exception(f\"{log_prefix}: Error after {duration_ms}ms: {e}\")\n            return ArbiterSynthesisResult(error=str(e), needs_iteration=False, duration_ms=duration_ms, prompt_used=arbiter_prompt_text) \n\n    def _parse_arbiter_response(self, text: str) -> ArbiterSynthesisResult:\n        logger.debug(\"Parsing arbiter response text.\")\n        result = ArbiterSynthesisResult() \n        patterns = {\"synthesis\": r\"<synthesis>([sS]*?)</synthesis>\", \"confidence\": r\"<confidence>\\s*(\\d*\\.?\\d+)\\s*</confidence>\", \"analysis\": r\"<analysis>([sS]*?)</analysis>\", \"dissent\": r\"<dissent>([sS]*?)</dissent>\", \"needs_iteration\": r\"<needs_iteration>\\s*(true|false)\\s*</needs_iteration>\", \"refinement_areas\": r\"<refinement_areas>([sS]*?)</refinement_areas>\"}\n        for key, pattern in patterns.items():\n            match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)\n            if match:\n                content = match.group(1).strip()\n                try:\n                    if key == \"confidence\": result.confidence = float(content)\n                    elif key == \"needs_iteration\": result.needs_iteration = content.lower() == \"true\"\n                    elif key == \"refinement_areas\":\n                        area_matches = re.findall(r\"<area>([^<]+)</area>\", content, re.IGNORECASE | re.DOTALL); result.refinement_areas = [a.strip() for a in area_matches if a.strip()] or ([a.strip().lstrip('- ') for a in content.split(\"\\n\") if a.strip()] if content else [])\n                    else: setattr(result, key, content)\n                except Exception as e: logger.warning(f\"Error parsing arbiter field '{key}': {e}\")\n            elif key in [\"synthesis\", \"confidence\", \"needs_iteration\"]: logger.warning(f\"Mandatory tag '{key}' missing in arbiter response.\")\n            else: logger.debug(f\"Optional tag '{key}' not found.\")\n        return result\n\n# --- Saved Consortium Model ---\n\nclass ConsortiumModel(llm.Model):\n    can_stream = False \n    class Options(llm.Options): confidence_threshold: Optional[float]=None; max_iterations: Optional[int]=None; minimum_iterations: Optional[int]=None; system_prompt: Optional[str]=None\n    def __init__(self, model_id: str, config: ConsortiumConfig): self.model_id = model_id; self.saved_config = config \n    def __str__(self): return f\"Consortium Model: {self.model_id}\"\n    def execute(self, prompt: llm.Prompt, stream, response: llm.Response, conversation):\n        run_id = str(uuid.uuid4()); start_time = time.perf_counter(); logger.info(f\"Executing saved consortium '{self.model_id}' Run ID: {run_id}\")\n        current_config = self.saved_config.model_copy(deep=True); options_dict = prompt.options.model_dump(exclude_unset=True) if hasattr(prompt, 'options') and prompt.options else {}\n        current_config.confidence_threshold = options_dict.get('confidence_threshold', current_config.confidence_threshold)\n        current_config.max_iterations = options_dict.get('max_iterations', current_config.max_iterations)\n        current_config.minimum_iterations = options_dict.get('minimum_iterations', current_config.minimum_iterations)\n        current_config.system_prompt = options_dict.get('system_prompt', current_config.system_prompt)\n        if prompt.system: current_config.system_prompt = prompt.system\n        logger.debug(f\"Effective config run {run_id}: {current_config.model_dump(exclude_none=True)}\")\n        try:\n            orchestrator = ConsortiumOrchestrator(current_config)\n            result_payload = orchestrator.orchestrate(prompt.prompt, consortium_id=run_id) \n            duration_ms = int((time.perf_counter() - start_time) * 1000)\n            final_synthesis_text = result_payload.get(\"synthesis\", {}).get(\"synthesis\", \"N/A\")\n            overall_response_id = manual_log_response(model_name=self.model_id, prompt_text=prompt.prompt, system_prompt=current_config.system_prompt, response_text=final_synthesis_text, duration_ms=duration_ms, response_json=result_payload)\n            logger.debug(f\"Logged overall execution '{self.model_id}' resp ID {overall_response_id} (Run {run_id})\")\n            # We don't populate the passed 'response' object's fields directly here, \n            # manual_log_response handles the DB insert. The return value is what matters.\n            return final_synthesis_text\n        except Exception as e: logger.exception(f\"Consortium execute failed for '{self.model_id}' Run {run_id}\"); raise llm.ModelError(f\"Consortium failed: {e}\")\n\n# --- Configuration Management ---\n\ndef _get_consortium_configs() -> Dict[str, ConsortiumConfig]:\n    try:\n        db = DatabaseConnection.get_connection(); configs = {}\n        if \"consortium_configs\" in db.table_names():\n            for row in db[\"consortium_configs\"].rows:\n                try: configs[row[\"name\"]] = ConsortiumConfig.from_json(row[\"config\"])\n                except Exception as e: logger.error(f\"Error loading config '{row['name']}': {e}\")\n        else: logger.warning(\"Table 'consortium_configs' not found.\")\n        return configs\n    except Exception as e: logger.error(f\"Failed loading configs: {e}\"); return {}\n\ndef _save_consortium_config(name: str, config: ConsortiumConfig) -> None:\n    db = DatabaseConnection.get_connection(); db[\"consortium_configs\"].insert({\"name\": name, \"config\": config.to_json()}, pk=\"name\", replace=True); logger.info(f\"Saved consortium '{name}'.\")\n\n# --- CLI Commands ---\n\nfrom click_default_group import DefaultGroup\nclass DefaultToRunGroup(DefaultGroup):\n     def __init__(self, *args, **kwargs): kwargs.setdefault('default_if_no_args', True); kwargs.setdefault('default', 'run'); super().__init__(*args, **kwargs)\n\n@llm.hookimpl\ndef register_commands(cli):\n    @cli.group(cls=DefaultToRunGroup, default='run', default_if_no_args=True)\n    def consortium(): \"\"\"Manage and run LLM Consortiums.\"\"\"\n    @consortium.command(name=\"run\")\n    @click.argument(\"prompt\", required=False)\n    @click.option(\"-m\", \"--model\", \"models\", multiple=True, help=\"Model & count ('name:count').\", default=[\"claude-3.5-sonnet:1\"])\n    @click.option(\"--arbiter\", help=\"Arbiter model.\", default=\"claude-3.5-sonnet\")\n    @click.option(\"--confidence-threshold\", type=click.FloatRange(0, 1), default=0.8)\n    @click.option(\"--max-iterations\", type=int, default=3); @click.option(\"--min-iterations\", type=int, default=1)\n    @click.option(\"--system\", help=\"System prompt override.\")\n    @click.option(\"--output\", type=click.Path(dir_okay=False, writable=True), help=\"Save full JSON results.\")\n    @click.option(\"--stdin/--no-stdin\", default=True, help=\"Read extra input from stdin.\")\n    @click.option(\"--raw\", is_flag=True, help=\"Output detailed execution trace (JSON).\")\n    def run_command(prompt, models, arbiter, confidence_threshold, max_iterations, min_iterations, system, output, stdin, raw):\n        \"\"\"Run a prompt through a new consortium.\"\"\"\n        model_dict = {}\n        for m_spec in models:\n            parts=m_spec.split(':',1); name=parts[0]; count=1\n            if len(parts)==2: \n                 try: count=int(parts[1]); assert count >= 1\n                 except (ValueError,AssertionError): raise click.ClickException(f\"Invalid count:'{m_spec}'\")\n            model_dict[name]=count\n        cli_prompt=prompt; final_prompt=None\n        if not cli_prompt and stdin: final_prompt=read_stdin_if_not_tty(); assert final_prompt,\"No prompt.\"\n        elif cli_prompt: final_prompt=cli_prompt; final_prompt = f\"{final_prompt}\\n\\n{stdin_content}\" if stdin and (stdin_content := read_stdin_if_not_tty()) else final_prompt\n        else: raise click.UsageError(\"No prompt.\")\n        config = ConsortiumConfig(models=model_dict, arbiter=arbiter, system_prompt=system, confidence_threshold=confidence_threshold, max_iterations=max_iterations, minimum_iterations=min_iterations)\n        orchestrator = ConsortiumOrchestrator(config)\n        try:\n            result_payload = orchestrator.orchestrate(final_prompt) \n            if output: try: with open(output,'w') as f: json.dump(result_payload,f,indent=2); logger.info(f\"Results saved: {output}\")\n                      except Exception as e: logger.error(f\"Failed writing output: {e}\")\n            final_synthesis = result_payload.get(\"synthesis\", {}).get(\"synthesis\", \"No synthesis found.\")\n            click.echo(final_synthesis)\n            if raw: click.echo(\"\\n--- Execution Trace ---\\n\" + json.dumps(result_payload, indent=2)) \n        except Exception as e: logger.exception(\"Run command failed.\"); raise click.ClickException(f\"Failed: {e}\")\n\n    @consortium.command() @click.argument(\"name\")\n    @click.option(\"-m\",\"--model\",\"models\",multiple=True,required=True,help=\"Model&count('name:count').\")\n    @click.option(\"--arbiter\",required=True); @click.option(\"--confidence-threshold\",type=click.FloatRange(0,1),default=0.8)\n    @click.option(\"--max-iterations\",type=int,default=3); @click.option(\"--min-iterations\",type=int,default=1)\n    @click.option(\"--system\",help=\"System prompt.\")\n    def save(name, models, arbiter, confidence_threshold, max_iterations, min_iterations, system):\n        \"\"\"Save consortium settings.\"\"\"\n        model_dict = {}\n        for m_spec in models: parts=m_spec.split(':',1); name_part=parts[0]; count=1; \n             if len(parts)==2: try: count=int(parts[1]); assert count >= 1; except Exception: raise click.ClickException(f\"Invalid count: {m_spec}\")\n             model_dict[name_part]=count\n        if not model_dict: raise click.ClickException(\"No models.\")\n        config = ConsortiumConfig(models=model_dict, arbiter=arbiter, system_prompt=system, confidence_threshold=confidence_threshold, max_iterations=max_iterations, minimum_iterations=min_iterations)\n        try: _save_consortium_config(name, config); click.echo(f\"Consortium '{name}' saved.\")\n        except Exception as e: raise click.ClickException(f\"Save failed: {e}\")\n\n    @consortium.command(\"list\")\n    def list_configs(): \"\"\"List saved consortiums.\"\"\"\n        configs = _get_consortium_configs(); \n        if not configs: click.echo(\"No saved consortiums.\"); return\n        click.echo(\"Saved Consortiums:\"); \n        for name, cfg in configs.items(): click.echo(f\"- {name}: Arbiter={cfg.arbiter}, Models={cfg.models}, Conf={cfg.confidence_threshold:.2f}, Iter={cfg.minimum_iterations}-{cfg.max_iterations}\")\n\n    @consortium.command() @click.argument(\"name\")\n    def remove(name): \"\"\"Remove a saved consortium.\"\"\"\n        try:\n            db = DatabaseConnection.get_connection(); \n            if \"consortium_configs\" not in db.table_names(): raise click.ClickException(\"Config table missing.\")\n            count = db[\"consortium_configs\"].delete_where(\"name = ?\", [name])\n            if count > 0: click.echo(f\"Consortium '{name}' removed.\")\n            else: raise click.ClickException(f\"Consortium '{name}' not found.\")\n        except Exception as e: logger.error(f\"Failed remove '{name}': {e}\"); raise click.ClickException(f\"Remove failed: {e}\")\n\n@llm.hookimpl\ndef register_models(register):\n    \"\"\"Register saved consortiums as callable models.\"\"\"\n    logger.debug(\"Registering saved consortium models.\"); configs = _get_consortium_configs()\n    for name, config in configs.items():\n        try: register(ConsortiumModel(name, config), aliases=(name,)); logger.debug(f\"Registered: {name}\")\n        except Exception as e: logger.error(f\"Failed register '{name}': {e}\")\n\n# --- Plugin Metadata ---\nclass LLMConsortiumPlugin: __version__ = \"0.6.0\" # Manual logging implementation\nlogger.debug(\"llm_consortium module init complete.\"); \n__all__ = [ 'ConsortiumOrchestrator', 'ConsortiumConfig', 'ConsortiumModel', 'register_commands', 'register_models' ]\n</file>\n</WRITE_FILES>",
    "response_json": {
      "candidates": [
        {
          "finishReason": "STOP",
          "index": 0,
          "safetyRatings": [
            {
              "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
              "probability": "NEGLIGIBLE"
            },
            {
              "category": "HARM_CATEGORY_HATE_SPEECH",
              "probability": "NEGLIGIBLE"
            },
            {
              "category": "HARM_CATEGORY_HARASSMENT",
              "probability": "NEGLIGIBLE"
            },
            {
              "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
              "probability": "NEGLIGIBLE"
            }
          ]
        }
      ],
      "modelVersion": "gemini-2.5-pro-exp-03-25"
    },
    "conversation_id": "01jrncrhj8bgetc5a3wapezz9c",
    "duration_ms": 134306,
    "datetime_utc": "2025-04-12T16:49:27.418264+00:00",
    "input_tokens": 178297,
    "output_tokens": 11497,
    "token_details": "{\"promptTokensDetails\": [{\"modality\": \"TEXT\", \"tokenCount\": 178297}], \"thoughtsTokenCount\": 621}",
    "conversation_name": "<COMMAND>pwd tree </COMMAND> /h\u2026",
    "conversation_model": "anthropic/claude-3-7-sonnet-latest",
    "schema_json": null,
    "prompt_fragments": [],
    "system_fragments": [],
    "attachments": []
  }
]
